{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d14c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 2.0.1\n",
      "Pytorch version: 2.5.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: /home/<username>/miniconda3/envs/medical/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: 0.25.0\n",
      "scipy version: 1.15.3\n",
      "Pillow version: 11.1.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.20.1\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from skimage.metrics import structural_similarity as ssim_fn\n",
    "import wandb\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a0baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedBrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split=\"train\", seed=42):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"split must be 'train', 'val' or 'test'\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.seed = seed\n",
    "        self.samples = self._create_file_pairs()\n",
    "        self._split_dataset()\n",
    "\n",
    "    def _create_file_pairs(self):\n",
    "        t1_dir = os.path.join(self.root_dir, \"t1\")\n",
    "        t2_dir = os.path.join(self.root_dir, \"t2\")\n",
    "\n",
    "        t1_files = set(os.listdir(t1_dir))\n",
    "        t2_files = set(os.listdir(t2_dir))\n",
    "        common_files = list(t1_files.intersection(t2_files))\n",
    "        common_files.sort()\n",
    "\n",
    "        pairs = [(os.path.join(t1_dir, fname), os.path.join(t2_dir, fname)) for fname in common_files]\n",
    "        return pairs\n",
    "\n",
    "    def _split_dataset(self):\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "        n_total = len(self.samples)\n",
    "        n_train = int(n_total * 0.80)\n",
    "        n_val = int(n_total * 0.05)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.samples = self.samples[:n_train]\n",
    "        elif self.split == \"val\":\n",
    "            self.samples = self.samples[n_train:n_train + n_val]\n",
    "        elif self.split == \"test\":\n",
    "            self.samples = self.samples[n_train + n_val:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path, t2_path = self.samples[idx]\n",
    "        t1_image = Image.open(t1_path).convert(\"L\")\n",
    "        t2_image = Image.open(t2_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            t1_image = self.transform(t1_image)\n",
    "            t2_image = self.transform(t2_image)\n",
    "\n",
    "        return {\n",
    "            \"t1\": t1_image,\n",
    "            \"t2\": t2_image,\n",
    "            \"filename\": os.path.basename(t1_path)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839e1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = DATAPATH = '/home/andrea_moschetto/flow_matching_t1t2/data'\n",
    "OUTPUT_DIR = \"/home/andrea_moschetto/flow_matching_t1t2/outputs\"\n",
    "CHECKPOINTS_PATH = '/home/andrea_moschetto/flow_matching_t1t2/baseline_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334e7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c70e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percnorm(arr, lperc=5, uperc=99.5):\n",
    "    \"\"\"\n",
    "    Remove outlier intensities from a brain component,\n",
    "    similar to Tukey's fences method.\n",
    "    \"\"\"\n",
    "    upperbound = np.percentile(arr, uperc)\n",
    "    lowerbound = np.percentile(arr, lperc)\n",
    "    arr[arr > upperbound] = upperbound\n",
    "    arr[arr < lowerbound] = lowerbound\n",
    "    return arr\n",
    "\n",
    "def normalize(img):\n",
    "    # img: [C, H, W]\n",
    "    img = (img - img.min())/ (img.max() - img.min() + 1e-8)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d410fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINTS_PATH):\n",
    "    os.makedirs(CHECKPOINTS_PATH)\n",
    "\n",
    "\n",
    "def train_diffusion(model: DiffusionModelUNet, inferer: DiffusionInferer, train_loader: DataLoader, val_loader: DataLoader, project: str, exp_name: str, notes: str, n_epochs: int = 10, lr : float = 1e-3, generation_steps: int = 100):\n",
    "    with wandb.init(\n",
    "        project=project,\n",
    "        name=exp_name,\n",
    "        notes=notes,\n",
    "        tags=[\"flow\", \"brain\", \"diffusion\"],\n",
    "        config={\n",
    "            'model': model.__class__.__name__,\n",
    "            'epochs': n_epochs,\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'num_workers': train_loader.num_workers,\n",
    "            'optimizer': 'Adam',\n",
    "            'learning_rate': lr,\n",
    "            'loss_function': 'MSELoss',\n",
    "            'generation_steps': generation_steps,\n",
    "            'device': str(torch.cuda.get_device_name(0)\n",
    "                          if torch.cuda.is_available() else \"CPU\"),\n",
    "        }\n",
    "    ) as run:\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using\", torch.cuda.get_device_name(0)\n",
    "                if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_path = None\n",
    "        start_time = time.time()\n",
    "        for e in trange(n_epochs, desc=\"Epochs\"):\n",
    "            start_e_time = time.time()\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            for batch in train_loader:\n",
    "                t2_targets = batch[\"t2\"].to(device)  # [B, 1, H, W]\n",
    "                t1_cond = batch[\"t1\"].to(device)  # [B, 1, H, W]  # torch.randn_like(x_1).to(device)  # [B, 1, H, W]\n",
    "                noise = torch.randn_like(t2_targets).to(device)\n",
    "\n",
    "                \n",
    "                B = t2_targets.shape[0]\n",
    "                # Create timesteps\n",
    "                timesteps = torch.randint(0, generation_steps, (B,), device=t2_targets.device).long()\n",
    "                optimizer.zero_grad()\n",
    "                # Get model prediction\n",
    "                noise_pred = inferer(inputs=t2_targets, diffusion_model=model, noise=noise, timesteps=timesteps, condition=t1_cond, mode='concat')\n",
    "\n",
    "                loss = criterion(noise_pred.float(), noise.float())\n",
    "                train_losses.append(loss.item())\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            wandb.log({\"train_loss\": sum(train_losses) / len(train_losses)})\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    t2_targets = batch[\"t2\"].to(device)\n",
    "                    t1_cond = batch[\"t1\"].to(device)\n",
    "                    noise = torch.randn_like(t2_targets).to(device)\n",
    "                    B = t2_targets.shape[0]\n",
    "                    timesteps = torch.randint(0, generation_steps, (B,), device=t2_targets.device).long()\n",
    "                    noise_pred = inferer(inputs=t2_targets, diffusion_model=model, noise=noise, timesteps=timesteps, condition=t1_cond, mode='concat')\n",
    "                    val_loss = criterion(noise_pred.float(), noise.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "            batch_val_loss = sum(val_losses) / len(val_losses)\n",
    "            wandb.log({\"val_loss\": batch_val_loss})\n",
    "            e_time = time.time() - start_e_time\n",
    "            wandb.log({\"epoch_time_minutes\": e_time // 60})\n",
    "\n",
    "\n",
    "            # Checkpoint\n",
    "            if batch_val_loss < best_val_loss:\n",
    "                sample_batch = next(iter(val_loader))  # just one batch\n",
    "                t1_cond = sample_batch[\"t1\"][0].unsqueeze(0).to(device)\n",
    "                t2_target = sample_batch[\"t2\"][0].unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    gen_image = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=inferer.scheduler, mode='concat', conditioning=t1_cond)\n",
    "                    images = torch.cat([t1_cond, t2_target, normalize(gen_image)], dim=0)\n",
    "                    grid = torchvision.utils.make_grid(images, nrow=3)\n",
    "                    wandb.log({\n",
    "                        \"generation\": [wandb.Image(grid, caption=f\"Epoch {e+1}\")]\n",
    "                    })\n",
    "                \n",
    "                path = f'{CHECKPOINTS_PATH}/checkpoint_{exp_name}_{e+1}.pth'\n",
    "                torch.save({\n",
    "                    'epoch': e+1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, path)\n",
    "                if best_model_path is not None and os.path.exists(best_model_path):\n",
    "                    os.remove(best_model_path)\n",
    "                best_model_path = path\n",
    "                best_val_loss = batch_val_loss\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        wandb.log({\"total_running_hours\": elapsed_time // 3600})\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abd2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_and_save_predictions(model: nn.Module, inferer: DiffusionInferer, test_loader: DataLoader, device: str, output_dir: str = OUTPUT_DIR, just_one_batch: bool = False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "        t1_cond = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2_target = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        noise = torch.randn_like(t2_target).to(device)          # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        gen_image = inferer.sample(input_noise=noise, diffusion_model=model,\n",
    "                                   scheduler=inferer.scheduler, mode='concat', conditioning=t1_cond)\n",
    "\n",
    "        for i in range(t1_cond.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1_cond[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2_target[i].cpu(),\n",
    "                \"prediction\": gen_image[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "        if just_one_batch:\n",
    "            break\n",
    "\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb3014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        super().__init__()\n",
    "        self.directory = directory\n",
    "        self.files = sorted([\n",
    "            f for f in os.listdir(directory) if f.endswith('.pt')\n",
    "        ])\n",
    "        if not self.files:\n",
    "            raise ValueError(f\"No .pt files found in directory: {directory}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.directory, self.files[idx])\n",
    "        data = torch.load(file_path)\n",
    "        pred = data[\"prediction\"]       # expected shape: [1, H, W] or [C, H, W]\n",
    "        gt = data[\"target\"]\n",
    "        return pred, gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26f475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssim_from_dataset(dataset):\n",
    "    ssim_scores = []\n",
    "    mse_scores = []\n",
    "\n",
    "    example_pred = None\n",
    "    example_gt = None\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        pred, gt = dataset[i]  # tensors [1, H, W]\n",
    "\n",
    "        # Convert to numpy and squeeze channel\n",
    "        pred_np = pred.squeeze().cpu().numpy()\n",
    "        gt_np = gt.squeeze().cpu().numpy()\n",
    "\n",
    "        pred_np = normalize(percnorm(pred_np))\n",
    "        gt_np = normalize(percnorm(gt_np))\n",
    "\n",
    "        # Compute SSIM\n",
    "        ssim_val = ssim_fn(pred_np, gt_np, data_range=1.0)\n",
    "        ssim_scores.append(ssim_val)\n",
    "\n",
    "        # Compute MSE\n",
    "        mse_val = F.mse_loss(pred, gt).item()\n",
    "        mse_scores.append(mse_val)\n",
    "\n",
    "        # Store one example for visualization\n",
    "        if i == 4 and example_pred is None:\n",
    "            example_pred = pred_np\n",
    "            example_gt = gt_np\n",
    "\n",
    "    ssim_scores = np.array(ssim_scores)\n",
    "    mse_scores = np.array(mse_scores)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        \"Metric\": [\"SSIM\", \"MSE\"],\n",
    "        \"Mean\": [ssim_scores.mean(), mse_scores.mean()],\n",
    "        \"Variance\": [ssim_scores.var(), mse_scores.var()]\n",
    "    })\n",
    "\n",
    "    # Visualize example\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axs[0].imshow(example_gt, cmap='gray')\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(example_pred, cmap='gray')\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Example Comparison\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5aa11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "num_train_timesteps = 1000\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=2,\n",
    "    out_channels=1\n",
    ")\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps)\n",
    "inferer = DiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27aa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250526_230302-hgma385g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/FlowMatching-Baselines/runs/hgma385g' target=\"_blank\">diffusion-t1t2-brain150e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/FlowMatching-Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/FlowMatching-Baselines' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/FlowMatching-Baselines</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/FlowMatching-Baselines/runs/hgma385g' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/FlowMatching-Baselines/runs/hgma385g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.33it/s]\n",
      "Epochs:   1%|▎                                                       | 1/150 [04:16<10:36:22, 256.26s/it]"
     ]
    }
   ],
   "source": [
    "exp_name = \"diffusion-t1t2-brain150e\"\n",
    "train_diffusion(\n",
    "    model=model,\n",
    "    inferer=inferer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project=\"FlowMatching-Baselines\",\n",
    "    exp_name=exp_name,\n",
    "    notes=\"Training a diffusion model for T1-T2 brain image generation.\",\n",
    "    n_epochs=150,\n",
    "    lr=1e-4,\n",
    "    generation_steps=num_train_timesteps\n",
    ")\n",
    "generate_and_save_predictions(\n",
    "    model=model,\n",
    "    inferer=inferer,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    output_dir=f'{OUTPUT_DIR}/{exp_name}',\n",
    ")\n",
    "out_dataset = PredictionDataset(f'{OUTPUT_DIR}/{exp_name}')\n",
    "with wandb.init(\n",
    "    project = 'FlowMatching-Baselines',\n",
    "    name=f'evaluation-{exp_name}',\n",
    "    notes=\"Evaluation of the diffusion model on the test set.\",\n",
    "):\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    wandb.log({\"eval/metrics\": wandb.Table(dataframe=summary)})\n",
    "    wandb.log({\"eval/ssim_mean\": summary[\"Mean\"][0]})\n",
    "    wandb.log({\"eval/mse_mean\": summary[\"Mean\"][1]})\n",
    "    wandb.log({\"eval/ssim_var\": summary[\"Variance\"][0]})\n",
    "    wandb.log({\"eval/mse_var\": summary[\"Variance\"][1]})\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
