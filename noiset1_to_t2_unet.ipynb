{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407529e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim_fn\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fee7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '/home/andrea_moschetto/FlowMatching-MREConversion/data'\n",
    "OUTPUT_DIR = \"/home/andrea_moschetto/FlowMatching-MREConversion/outputs\"\n",
    "CHECKPOINTS_PATH = '/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2823cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedBrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split=\"train\", seed=42):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"split must be 'train', 'val' or 'test'\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.seed = seed\n",
    "        self.samples = self._create_file_pairs()\n",
    "        self._split_dataset()\n",
    "\n",
    "    def _create_file_pairs(self):\n",
    "        t1_dir = os.path.join(self.root_dir, \"t1\")\n",
    "        t2_dir = os.path.join(self.root_dir, \"t2\")\n",
    "\n",
    "        t1_files = set(os.listdir(t1_dir))\n",
    "        t2_files = set(os.listdir(t2_dir))\n",
    "        common_files = list(t1_files.intersection(t2_files))\n",
    "        common_files.sort()\n",
    "\n",
    "        pairs = [(os.path.join(t1_dir, fname), os.path.join(t2_dir, fname)) for fname in common_files]\n",
    "        return pairs\n",
    "\n",
    "    def _split_dataset(self):\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "        n_total = len(self.samples)\n",
    "        n_train = int(n_total * 0.80)\n",
    "        n_val = int(n_total * 0.05)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.samples = self.samples[:n_train]\n",
    "        elif self.split == \"val\":\n",
    "            self.samples = self.samples[n_train:n_train + n_val]\n",
    "        elif self.split == \"test\":\n",
    "            self.samples = self.samples[n_train + n_val:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path, t2_path = self.samples[idx]\n",
    "        t1_image = Image.open(t1_path).convert(\"L\")\n",
    "        t2_image = Image.open(t2_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            t1_image = self.transform(t1_image)\n",
    "            t2_image = self.transform(t2_image)\n",
    "\n",
    "        return {\n",
    "            \"t1\": t1_image,\n",
    "            \"t2\": t2_image,\n",
    "            \"filename\": os.path.basename(t1_path)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab03501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_step(model: DiffusionModelUNet, x_t: Tensor, t_start: Tensor, t_end: Tensor):\n",
    "    # delta_t shape (B, 1, 1, 1)\n",
    "    delta_t = (t_end - t_start).view(-1, 1, 1, 1)\n",
    "    \n",
    "    # model si aspetta t come tensor (B,)\n",
    "    v_hat = model(x_t, t_start)\n",
    "    \n",
    "    x_t_noise = x_t[:, 0:1,:, :] # [B, 1, H, W]\n",
    "    x_t_cond = x_t[:, 1:2, :, :] # [B, 1, H, W], che Ã¨ T1\n",
    "    \n",
    "    x_next_noise = x_t_noise + delta_t * v_hat\n",
    "    \n",
    "    x_next = torch.cat([x_next_noise, x_t_cond], dim=1) # [B, 2, H, W]\n",
    "    return x_next\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model: nn.Module, x_cond: Tensor, n_steps: int = 20):\n",
    "    model.eval()\n",
    "    \n",
    "    device = x_cond.device\n",
    "    batch_size = x_cond.shape[0]\n",
    "    \n",
    "    time_steps = torch.linspace(0.0, 1.0, n_steps + 1, device=device, dtype=torch.float32)\n",
    "    \n",
    "    x = torch.cat([torch.randn_like(x_cond,device=device), x_cond], dim=1) # [B, 2, H, W]\n",
    "    for i in range(n_steps):\n",
    "        t_start = time_steps[i].expand(batch_size)\n",
    "        t_end = time_steps[i + 1].expand(batch_size)\n",
    "        x = euler_step(model, x_t=x, t_start=t_start, t_end=t_end)\n",
    "    \n",
    "    return x[:, 0:1, :, :] # [B, 1, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e67e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percnorm(arr, lperc=5, uperc=99.5):\n",
    "    \"\"\"\n",
    "    Remove outlier intensities from a brain component,\n",
    "    similar to Tukey's fences method.\n",
    "    \"\"\"\n",
    "    upperbound = np.percentile(arr, uperc)\n",
    "    lowerbound = np.percentile(arr, lperc)\n",
    "    arr[arr > upperbound] = upperbound\n",
    "    arr[arr < lowerbound] = lowerbound\n",
    "    return arr\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    # img: [C, H, W]\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(in_tensor_img):\n",
    "    # tensor_img: [1, H, W]\n",
    "    img_np = in_tensor_img.squeeze(0).cpu().numpy()  # [H, W]\n",
    "    # Percentile-based normalization\n",
    "    img_np = percnorm(img_np)\n",
    "    out_tensor_image = torch.from_numpy(\n",
    "        img_np).unsqueeze(0)  # Back to [1, H, W]\n",
    "    out_tensor_image = normalize(\n",
    "        out_tensor_image)            # 0-1 normalization\n",
    "    return out_tensor_image.to(in_tensor_img.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a03b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_generation(model, epoch, device, n_steps: int, reference_image=None, use_wandb=True):\n",
    "    with torch.no_grad():\n",
    "        if reference_image is not None:\n",
    "            reference_t1, reference_t2 = reference_image  # t1 = input, t2 = ground truth\n",
    "            x_cond = reference_t1.to(device)                 # input al modello = t1\n",
    "            x_gen = generate(model=model, x_cond=x_cond, n_steps=n_steps)\n",
    "\n",
    "            # Visualizza t1 (input), t2 (vero), x_gen (generato)\n",
    "            images = torch.cat([reference_t1, reference_t2, x_gen], dim=0)  # [3, 1, H, W]\n",
    "        else:\n",
    "            raise ValueError(\"reference_image must be provided when generating from t1.\")\n",
    "\n",
    "        grid = torchvision.utils.make_grid(images, nrow=3, normalize=True)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"generation\": [wandb.Image(grid, caption=f\"Epoch {epoch+1}\")]\n",
    "            })\n",
    "\n",
    "        return grid, x_gen\n",
    "\n",
    "\n",
    "\n",
    "def show_grid(grid):\n",
    "    # grid Ã¨ un tensore [C, H, W], lo trasformiamo in un'immagine visualizzabile\n",
    "    np_grid = grid.permute(1, 2, 0).cpu().numpy()  # da [C,H,W] a [H,W,C]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np_grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debe8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{CHECKPOINTS_PATH}/backups'):\n",
    "    os.makedirs(f'{CHECKPOINTS_PATH}/backups')\n",
    "\n",
    "\n",
    "def train_flow(model: DiffusionModelUNet, device: str, train_loader: DataLoader, val_loader: DataLoader, project: str, exp_name: str, notes: str, n_epochs: int = 10, lr : float = 1e-3, generation_steps: int = 100):\n",
    "    with wandb.init(\n",
    "        project=project,\n",
    "        name=exp_name,\n",
    "        notes=notes,\n",
    "        tags=[\"flow\", \"brain\", \"diffusion\"],\n",
    "        config={\n",
    "            'model': model.__class__.__name__,\n",
    "            'epochs': n_epochs,\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'num_workers': train_loader.num_workers,\n",
    "            'optimizer': 'Adam',\n",
    "            'learning_rate': f'{lr} -> 1e-6',\n",
    "            'loss_function': 'MSELoss',\n",
    "            'generation_steps': generation_steps,\n",
    "            'device': str(torch.cuda.get_device_name(0)\n",
    "                          if torch.cuda.is_available() else \"CPU\"),\n",
    "        }\n",
    "    ) as run:\n",
    "        print(\"Using\", torch.cuda.get_device_name(0)\n",
    "                if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_path = None\n",
    "        start_time = time.time()\n",
    "        for e in trange(n_epochs, desc=\"Epochs\"):\n",
    "            start_e_time = time.time()\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            for batch in tqdm(train_loader, desc=f\"Training epoch {e}\"):\n",
    "                x_1 = batch[\"t2\"].to(device)  # [B, 1, H, W]\n",
    "                x_0_cond = batch[\"t1\"].to(device)  # [B, 1, H, W]  # torch.randn_like(x_1).to(device)  # [B, 1, H, W]\n",
    "                x_0_noise = torch.randn_like(x_0_cond).to(device)  # [B, 1, H, W]\n",
    "                \n",
    "                # add the corresponding t1 to the second channel of x_0\n",
    "                B = x_0_cond.shape[0]\n",
    "                t = torch.rand(B, device=device)  # B \n",
    "                t_img = t.view(B, 1, 1, 1)  # [B, 1, 1, 1] for broadcasting\n",
    "\n",
    "                x_t = (1 - t_img) * x_0_noise + t_img * x_1  # [B, 1, H, W]\n",
    "                x_t = torch.cat([x_t, x_0_cond], dim=1)  # [B, 2, H, W]\n",
    "                \n",
    "                dx_t = x_1 - x_0_noise  # [B, 1, H, W]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(x_t, t)  # [B, 1, H, W]\n",
    "                loss = criterion(pred, dx_t)\n",
    "                train_losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            wandb.log({\"train_loss\": sum(train_losses) / len(train_losses)})\n",
    "            wandb.log({\"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_1 = batch[\"t2\"].to(device)\n",
    "                    x_0_cond = batch[\"t1\"].to(device)\n",
    "                    x_0_noise = torch.randn_like(x_0_cond).to(device)  # [B, 1, H, W]\n",
    "                    B = x_0_cond.shape[0]\n",
    "                    t = torch.rand(B, device=device)\n",
    "                    t_img = t.view(B, 1, 1, 1)\n",
    "                    x_t = (1 - t_img) * x_0_noise + t_img * x_1\n",
    "                    x_t = torch.cat([x_t, x_0_cond], dim=1)  # [B, 2, H, W]\n",
    "                    dx_t = x_1 - x_0_noise\n",
    "\n",
    "                    pred = model(x_t, t)\n",
    "                    val_loss = criterion(pred, dx_t)\n",
    "                    val_losses.append(val_loss.item())\n",
    "            batch_val_loss = sum(val_losses) / len(val_losses)\n",
    "            wandb.log({\"val_loss\": batch_val_loss})\n",
    "            e_time = time.time() - start_e_time\n",
    "            wandb.log({\"epoch_time_minutes\": e_time // 60})\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            # Checkpoint\n",
    "            if e % 5 == 0 or e == n_epochs-1 or batch_val_loss < best_val_loss:\n",
    "                sample_batch = next(iter(val_loader))  # just one batch\n",
    "                t1_gt = sample_batch[\"t1\"][0].unsqueeze(0).to(device)\n",
    "                t2_gt = sample_batch[\"t2\"][0].to(device)  # [1, 1, H, W]\n",
    "                t2_gen = generate(model=model, x_cond=t1_gt, n_steps=generation_steps)  # [1, 1, H, W]\n",
    "                imgs = torch.stack([t1_gt.squeeze(0), t2_gt, preprocess_image(t2_gen.squeeze(0))], dim=0)\n",
    "                grid = torchvision.utils.make_grid(imgs, nrow=3)\n",
    "                \n",
    "                if batch_val_loss < best_val_loss:\n",
    "                    path = f'{CHECKPOINTS_PATH}/checkpoint_{exp_name}_{e+1}_best.pth'\n",
    "                    torch.save({\n",
    "                        'epoch': e+1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, path)\n",
    "                    if best_model_path is not None and os.path.exists(best_model_path):\n",
    "                        os.remove(best_model_path)\n",
    "                    best_model_path = path\n",
    "                    best_val_loss = batch_val_loss\n",
    "                    wandb.log({\n",
    "                        \"best_model_generations\": [wandb.Image(grid, caption=f\"Epoch {e+1} - Best\")]\n",
    "                    })\n",
    "                else:\n",
    "                    path = f'{CHECKPOINTS_PATH}/backups/checkpoint_{exp_name}_{e+1}.pth'\n",
    "                    torch.save({\n",
    "                        'epoch': e+1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, path)\n",
    "                    wandb.log({\n",
    "                        \"each5e_generation\": [wandb.Image(grid, caption=f\"Epoch {e+1}\")]\n",
    "                    })\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        wandb.log({\"total_running_hours\": elapsed_time // 3600})\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "    print(\"Training complete.\")\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8b29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_and_save_predictions(model, test_loader, device, output_dir=OUTPUT_DIR):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        t1 = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2 = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        x_gen = generate(model, x_cond=t1, n_steps=300)\n",
    "        # print(t2.shape, x_gen.shape)\n",
    "\n",
    "        for i in range(t1.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2[i].cpu(),\n",
    "                \"prediction\": x_gen[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "        wandb.log({\"prediction_progress\": idx})\n",
    "\n",
    "    return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85105d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        super().__init__()\n",
    "        self.directory = directory\n",
    "        self.files = sorted([\n",
    "            f for f in os.listdir(directory) if f.endswith('.pt')\n",
    "        ])\n",
    "        if not self.files:\n",
    "            raise ValueError(f\"No .pt files found in directory: {directory}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.directory, self.files[idx])\n",
    "        data = torch.load(file_path)\n",
    "        pred = data[\"prediction\"]       # expected shape: [1, H, W] or [C, H, W]\n",
    "        gt = data[\"target\"]\n",
    "        return pred, gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efeb967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "\n",
    "def compute_ssim_from_dataset(dataset):\n",
    "    ssim_scores = []\n",
    "    mse_scores = []\n",
    "    ssim_masked_scores = []\n",
    "    psnr_scores = []\n",
    "\n",
    "    example_pred = None\n",
    "    example_gt = None\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        pred, gt = dataset[i]  # tensors [1, H, W]\n",
    "\n",
    "        # Convert to numpy and squeeze channel\n",
    "        pred_np = pred.squeeze().cpu().numpy()\n",
    "        gt_np = gt.squeeze().cpu().numpy()\n",
    "\n",
    "        pred_np = normalize(percnorm(pred_np))\n",
    "        gt_np = normalize(percnorm(gt_np))\n",
    "\n",
    "        threshold_triangle_gt = filters.threshold_triangle(gt_np)\n",
    "        mask = gt_np > threshold_triangle_gt\n",
    "        masked_gt = gt_np * mask\n",
    "\n",
    "        threshold_triangle_pred = filters.threshold_triangle(pred_np)\n",
    "        mask_pred = pred_np > threshold_triangle_pred\n",
    "        masked_pred = pred_np * mask_pred\n",
    "\n",
    "        # Compute SSIM\n",
    "        ssim_masked = ssim_fn(masked_gt, masked_pred, data_range=1.0)\n",
    "        ssim_masked_scores.append(ssim_masked)\n",
    "\n",
    "        ssim_val = ssim_fn(pred_np, gt_np, data_range=1.0)\n",
    "        ssim_scores.append(ssim_val)\n",
    "\n",
    "        # Compute MSE\n",
    "        mse_val = F.mse_loss(pred, gt).item()\n",
    "        mse_scores.append(mse_val)\n",
    "        \n",
    "        # Compute PSNR on masked images\n",
    "        psnr_val = psnr_fn(masked_gt, masked_pred, data_range=1.0)\n",
    "        psnr_scores.append(psnr_val)\n",
    "        \n",
    "\n",
    "        # Store one example for visualization\n",
    "        if i == 4 and example_pred is None:\n",
    "            example_pred = masked_pred\n",
    "            example_gt = masked_gt\n",
    "\n",
    "    ssim_scores = np.array(ssim_scores)\n",
    "    ssim_masked_scores = np.array(ssim_masked_scores)\n",
    "    mse_scores = np.array(mse_scores)\n",
    "    psnr_scores = np.array(psnr_scores)\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"Metric\": [\"SSIM\", \"MASKED_SSIM\", \"MSE\", \"PSNR\"],\n",
    "        \"Mean\": [ssim_scores.mean(), ssim_masked_scores.mean(), mse_scores.mean(), psnr_scores.mean()],\n",
    "        \"Variance\": [ssim_scores.var(), ssim_masked_scores.var(), mse_scores.var(), psnr_scores.var()],\n",
    "    })\n",
    "\n",
    "    # Visualize example\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axs[0].imshow(example_gt, cmap='gray')\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(example_pred, cmap='gray')\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Example Comparison\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad318d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,  # Â 2D\n",
    "    in_channels=2,  #  noise + t1 \n",
    "    out_channels=1  #  predice delta_x_t solo sul noise (condizionato da t1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f910d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5372/3463383609.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-noiset1t2-150e_137.pth', map_location=device)['model_state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-noiset1t2-150e_137.pth', map_location=device)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2e582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250522_140927-f9z9200i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i' target=\"_blank\">finetuning50e-unetflow-noiset1t2-150e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:29<00:00,  1.40it/s]\n",
      "Training epoch 1: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:43<00:00,  1.31it/s]\n",
      "Epochs:   4%|âââ                                                       | 2/50 [07:48<3:08:30, 235.63s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_name = \"finetuning50e-unetflow-noiset1t2-150e\"\n",
    "modelpath = train_flow(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader,\n",
    "    project='flowmatching-t1-to-t2', \n",
    "    exp_name=exp_name,\n",
    "    notes=\"UNet flow model for directional diffusion from noise+T1 to T2.\",\n",
    "    n_epochs=50, \n",
    "    lr=1e-7,\n",
    "    generation_steps=300)\n",
    "\n",
    "generate_and_save_predictions(model, test_loader, device,output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f\"evaluation-{exp_name}\",\n",
    "    notes=\"SSIM and MSE scores for the generated predictions.\"\n",
    "):\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    wandb.log({\"ssim-mse\": wandb.Table(dataframe=summary)})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the best checkpoint\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(modelpath, map_location=device)['model_state_dict'])\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250609_103742-er6704s0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/er6704s0' target=\"_blank\">evaluation-unetflow-noiset1t2-150e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/er6704s0' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/er6704s0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions:  47%|âââââââââââââââââââââ                       | 26/55 [32:47<36:47, 76.14s/it]"
     ]
    }
   ],
   "source": [
    "exp_name = \"unetflow-noiset1t2-150e\"\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f\"evaluation-{exp_name}\",\n",
    "    notes=\"SSIM and MSE scores for the generated predictions.\"\n",
    "):\n",
    "    generate_and_save_predictions(model, test_loader, device,output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    wandb.log({\"eval/metrics\": wandb.Table(dataframe=summary)}) # log the summary table\n",
    "    wandb.log({\"eval/ssim_mean\": summary[\"Mean\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_mean\": summary[\"Mean\"][1]})\n",
    "    wandb.log({\"eval/mse_mean\": summary[\"Mean\"][2]})\n",
    "    wandb.log({\"eval/ssim_var\": summary[\"Variance\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_var\": summary[\"Variance\"][1]})\n",
    "    wandb.log({\"eval/mse_var\": summary[\"Variance\"][2]})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e774f34",
   "metadata": {},
   "source": [
    "## Training with a Bigger Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332221b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,  # Â 2D\n",
    "    in_channels=2,  # x+noise\n",
    "    out_channels=1,  # predice delta_x_t\n",
    "    num_channels=(64, 128, 256, 256),\n",
    "    attention_levels=(False, False, True, True),\n",
    "    num_head_channels=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a00998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250617_013145-6fu4qc9m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/6fu4qc9m' target=\"_blank\">big-unetflow-noiset1t2-s300e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/6fu4qc9m' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/6fu4qc9m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                                                    | 0/300 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "exp_name = \"big-unetflow-noiset1t2-s300e\"\n",
    "best_modelpath = train_flow(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    exp_name=exp_name,\n",
    "    notes=\"Big UNet flow model for directional diffusion from T1+Noise to T2. 300 epochs.\",\n",
    "    n_epochs=300,\n",
    "    lr=5e-4,\n",
    "    generation_steps=300)\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load(best_modelpath, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f'evaluation-{exp_name}',\n",
    "    notes=\"Evaluation of the flow model on the test set.\",\n",
    "):\n",
    "    generate_and_save_predictions(model, test_loader, device, output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    # log the summary table\n",
    "    wandb.log({\"eval/ssim_mean\": summary[\"Mean\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_mean\": summary[\"Mean\"][1]})\n",
    "    wandb.log({\"eval/mse_mean\": summary[\"Mean\"][2]})\n",
    "    wandb.log({\"eval/psnr_mean\": summary[\"Mean\"][3]})\n",
    "    wandb.log({\"eval/ssim_var\": summary[\"Variance\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_var\": summary[\"Variance\"][1]})\n",
    "    wandb.log({\"eval/mse_var\": summary[\"Variance\"][2]})\n",
    "    wandb.log({\"eval/psnr_var\": summary[\"Variance\"][3]})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a4fc3",
   "metadata": {},
   "source": [
    "## Retrain a small Unet with scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701939d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,  # Â 2D\n",
    "    in_channels=2,  # x+noise\n",
    "    out_channels=1,  # predice delta_x_t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adddbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 48: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.30it/s]-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Training epoch 49: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.29it/s]\n",
      "Training epoch 50: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 51: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 52: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 53: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 54: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 55: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 56: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 57: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 58: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 59: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 60: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 61: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 62: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 63: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 64: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 65: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 66: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 67: 100%|âââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 122: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 123: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 124: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 125: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 126: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 127: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 128: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 129: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 130: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 131: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 132: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 133: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 134: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 135: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 136: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:41<00:00,  1.32it/s]\n",
      "Training epoch 137: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 138: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 139: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 140: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 141: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 142: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 143: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 144: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 145: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 146: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 147: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 148: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 149: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 150: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 151: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 152: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 153: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 154: 100%|ââââââââââââââââââââââââââââââââââââââââââââââ| 293/293 [03:41<00:00,  1.32it/s]\n",
      "Epochs:  52%|âââââââââââââââââââââââââââ                         | 155/300 [10:00:40<9:11:12, 228.09s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250619_143901-7mkr41o0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/7mkr41o0' target=\"_blank\">unetflow-noiset1t2-s300e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/7mkr41o0' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/7mkr41o0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                                                    | 0/300 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "exp_name = \"unetflow-noiset1t2-s300e\"\n",
    "best_modelpath = train_flow(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    exp_name=exp_name,\n",
    "    notes=\"Big UNet flow model for directional diffusion from T1+Noise to T2. 300 epochs.\",\n",
    "    n_epochs=300,\n",
    "    lr=3e-4,\n",
    "    generation_steps=300)\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load(best_modelpath, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f'evaluation-{exp_name}',\n",
    "    notes=\"Evaluation of the flow model on the test set.\",\n",
    "):\n",
    "    generate_and_save_predictions(model, test_loader, device, output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    # log the summary table\n",
    "    wandb.log({\"eval/ssim_mean\": summary[\"Mean\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_mean\": summary[\"Mean\"][1]})\n",
    "    wandb.log({\"eval/mse_mean\": summary[\"Mean\"][2]})\n",
    "    wandb.log({\"eval/psnr_mean\": summary[\"Mean\"][3]})\n",
    "    wandb.log({\"eval/ssim_var\": summary[\"Variance\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_var\": summary[\"Variance\"][1]})\n",
    "    wandb.log({\"eval/mse_var\": summary[\"Variance\"][2]})\n",
    "    wandb.log({\"eval/psnr_var\": summary[\"Variance\"][3]})\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
