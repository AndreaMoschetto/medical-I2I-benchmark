{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407529e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fee7f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrea_moschetto/flow_matching_t1t2/data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATAPATH = DATAPATH = '/home/andrea_moschetto/flow_matching_t1t2/data'\n",
    "OUTPUT_DIR = \"/home/andrea_moschetto/flow_matching_t1t2/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2823cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedBrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split=\"train\", seed=42):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"split must be 'train', 'val' or 'test'\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.seed = seed\n",
    "        self.samples = self._create_file_pairs()\n",
    "        self._split_dataset()\n",
    "\n",
    "    def _create_file_pairs(self):\n",
    "        t1_dir = os.path.join(self.root_dir, \"t1\")\n",
    "        t2_dir = os.path.join(self.root_dir, \"t2\")\n",
    "\n",
    "        t1_files = set(os.listdir(t1_dir))\n",
    "        t2_files = set(os.listdir(t2_dir))\n",
    "        common_files = list(t1_files.intersection(t2_files))\n",
    "        common_files.sort()\n",
    "\n",
    "        pairs = [(os.path.join(t1_dir, fname), os.path.join(t2_dir, fname)) for fname in common_files]\n",
    "        return pairs\n",
    "\n",
    "    def _split_dataset(self):\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "        n_total = len(self.samples)\n",
    "        n_train = int(n_total * 0.80)\n",
    "        n_val = int(n_total * 0.05)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.samples = self.samples[:n_train]\n",
    "        elif self.split == \"val\":\n",
    "            self.samples = self.samples[n_train:n_train + n_val]\n",
    "        elif self.split == \"test\":\n",
    "            self.samples = self.samples[n_train + n_val:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path, t2_path = self.samples[idx]\n",
    "        t1_image = Image.open(t1_path).convert(\"L\")\n",
    "        t2_image = Image.open(t2_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            t1_image = self.transform(t1_image)\n",
    "            t2_image = self.transform(t2_image)\n",
    "\n",
    "        return {\n",
    "            \"t1\": t1_image,\n",
    "            \"t2\": t2_image,\n",
    "            \"filename\": os.path.basename(t1_path)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab03501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_step(model: DiffusionModelUNet, x_t: Tensor, t_start: Tensor, t_end: Tensor):\n",
    "    # delta_t shape (B, 1, 1, 1)\n",
    "    delta_t = (t_end - t_start).view(-1, 1, 1, 1)\n",
    "    \n",
    "    # model si aspetta t come tensor (B,)\n",
    "    v_hat = model(x_t, t_start)\n",
    "    \n",
    "    x_t_noise = x_t[:, 0:1,:, :] # [B, 1, H, W]\n",
    "    x_t_cond = x_t[:, 1:2, :, :] # [B, 1, H, W], che è T1\n",
    "    \n",
    "    x_next_noise = x_t_noise + delta_t * v_hat\n",
    "    \n",
    "    x_next = torch.cat([x_next_noise, x_t_cond], dim=1) # [B, 2, H, W]\n",
    "    return x_next\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model: nn.Module, x_cond: Tensor, n_steps: int = 20):\n",
    "    model.eval()\n",
    "    \n",
    "    device = x_cond.device\n",
    "    batch_size = x_cond.shape[0]\n",
    "    \n",
    "    time_steps = torch.linspace(0.0, 1.0, n_steps + 1, device=device, dtype=torch.float32)\n",
    "    \n",
    "    x = torch.cat([torch.randn_like(x_cond,device=device), x_cond], dim=1) # [B, 2, H, W]\n",
    "    for i in range(n_steps):\n",
    "        t_start = time_steps[i].expand(batch_size)\n",
    "        t_end = time_steps[i + 1].expand(batch_size)\n",
    "        x = euler_step(model, x_t=x, t_start=t_start, t_end=t_end)\n",
    "    \n",
    "    return x[:, 0:1, :, :] # [B, 1, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a03b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_generation(model, epoch, device, n_steps: int, reference_image=None, use_wandb=True):\n",
    "    with torch.no_grad():\n",
    "        if reference_image is not None:\n",
    "            reference_t1, reference_t2 = reference_image  # t1 = input, t2 = ground truth\n",
    "            x_cond = reference_t1.to(device)                 # input al modello = t1\n",
    "            x_gen = generate(model=model, x_cond=x_cond, n_steps=n_steps)\n",
    "\n",
    "            # Visualizza t1 (input), t2 (vero), x_gen (generato)\n",
    "            images = torch.cat([reference_t1, reference_t2, x_gen], dim=0)  # [3, 1, H, W]\n",
    "        else:\n",
    "            raise ValueError(\"reference_image must be provided when generating from t1.\")\n",
    "\n",
    "        grid = torchvision.utils.make_grid(images, nrow=3, normalize=True)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"generation\": [wandb.Image(grid, caption=f\"Epoch {epoch+1}\")]\n",
    "            })\n",
    "\n",
    "        return grid, x_gen\n",
    "\n",
    "\n",
    "\n",
    "def show_grid(grid):\n",
    "    # grid è un tensore [C, H, W], lo trasformiamo in un'immagine visualizzabile\n",
    "    np_grid = grid.permute(1, 2, 0).cpu().numpy()  # da [C,H,W] a [H,W,C]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np_grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debe8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_PATH = '/home/andrea_moschetto/flow_matching_t1t2/checkpoints'\n",
    "if not os.path.exists(CHECKPOINTS_PATH):\n",
    "    os.makedirs(CHECKPOINTS_PATH)\n",
    "\n",
    "\n",
    "def train_flow(model: DiffusionModelUNet, device: str, train_loader: DataLoader, val_loader: DataLoader, project: str, exp_name: str, notes: str, n_epochs: int = 10, lr : float = 1e-3, generation_steps: int = 100):\n",
    "    with wandb.init(\n",
    "        project=project,\n",
    "        name=exp_name,\n",
    "        notes=notes,\n",
    "        tags=[\"flow\", \"brain\", \"diffusion\"],\n",
    "        config={\n",
    "            'model': model.__class__.__name__,\n",
    "            'epochs': n_epochs,\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'num_workers': train_loader.num_workers,\n",
    "            'optimizer': 'Adam',\n",
    "            'learning_rate': lr,\n",
    "            'loss_function': 'MSELoss',\n",
    "            'generation_steps': generation_steps,\n",
    "            'device': str(torch.cuda.get_device_name(0)\n",
    "                          if torch.cuda.is_available() else \"CPU\"),\n",
    "        }\n",
    "    ) as run:\n",
    "        print(\"Using\", torch.cuda.get_device_name(0)\n",
    "                if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_path = None\n",
    "        start_time = time.time()\n",
    "        for e in trange(n_epochs, desc=\"Epochs\"):\n",
    "            start_e_time = time.time()\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            for batch in tqdm(train_loader, desc=f\"Training epoch {e}\"):\n",
    "                x_1 = batch[\"t2\"].to(device)  # [B, 1, H, W]\n",
    "                x_0_cond = batch[\"t1\"].to(device)  # [B, 1, H, W]  # torch.randn_like(x_1).to(device)  # [B, 1, H, W]\n",
    "                x_0_noise = torch.randn_like(x_0_cond).to(device)  # [B, 1, H, W]\n",
    "                \n",
    "                # add the corresponding t1 to the second channel of x_0\n",
    "                B = x_0_cond.shape[0]\n",
    "                t = torch.rand(B, device=device)  # B \n",
    "                t_img = t.view(B, 1, 1, 1)  # [B, 1, 1, 1] for broadcasting\n",
    "\n",
    "                x_t = (1 - t_img) * x_0_noise + t_img * x_1  # [B, 1, H, W]\n",
    "                x_t = torch.cat([x_t, x_0_cond], dim=1)  # [B, 2, H, W]\n",
    "                \n",
    "                dx_t = x_1 - x_0_noise  # [B, 1, H, W]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(x_t, t)  # [B, 1, H, W]\n",
    "                loss = criterion(pred, dx_t)\n",
    "                train_losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            wandb.log({\"train_loss\": sum(train_losses) / len(train_losses)})\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_1 = batch[\"t2\"].to(device)\n",
    "                    x_0_cond = batch[\"t1\"].to(device)\n",
    "                    x_0_noise = torch.randn_like(x_0_cond).to(device)  # [B, 1, H, W]\n",
    "                    B = x_0_cond.shape[0]\n",
    "                    t = torch.rand(B, device=device)\n",
    "                    t_img = t.view(B, 1, 1, 1)\n",
    "                    x_t = (1 - t_img) * x_0_noise + t_img * x_1\n",
    "                    x_t = torch.cat([x_t, x_0_cond], dim=1)  # [B, 2, H, W]\n",
    "                    dx_t = x_1 - x_0_noise\n",
    "\n",
    "                    pred = model(x_t, t)\n",
    "                    val_loss = criterion(pred, dx_t)\n",
    "                    val_losses.append(val_loss.item())\n",
    "            batch_val_loss = sum(val_losses) / len(val_losses)\n",
    "            wandb.log({\"val_loss\": batch_val_loss})\n",
    "            e_time = time.time() - start_e_time\n",
    "            wandb.log({\"epoch_time_minutes\": e_time // 60})\n",
    "\n",
    "\n",
    "            # Checkpoint\n",
    "            if batch_val_loss < best_val_loss:\n",
    "                sample_batch = next(iter(val_loader))  # just one batch\n",
    "                reference_t2 = sample_batch[\"t2\"][0].unsqueeze(0).to(device)  # [1, 1, H, W]\n",
    "                reference_t1 = sample_batch[\"t1\"][0].unsqueeze(0).to(device)\n",
    "                reference = (reference_t1, reference_t2)\n",
    "                log_generation(model, epoch=e, device=device, n_steps=generation_steps, reference_image=reference)\n",
    "                \n",
    "                path = f'{CHECKPOINTS_PATH}/checkpoint_{exp_name}_{e+1}.pth'\n",
    "                torch.save({\n",
    "                    'epoch': e+1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, path)\n",
    "                if best_model_path is not None and os.path.exists(best_model_path):\n",
    "                    os.remove(best_model_path)\n",
    "                best_model_path = path\n",
    "                best_val_loss = batch_val_loss\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        wandb.log({\"total_running_hours\": elapsed_time // 3600})\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "    print(\"Training complete.\")\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd8b29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_and_save_predictions(model, test_loader, device, output_dir=OUTPUT_DIR):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    all_outputs = []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "        t1 = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2 = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        x_gen = generate(model, x_cond=t1, n_steps=300)\n",
    "        # print(t2.shape, x_gen.shape)\n",
    "\n",
    "        for i in range(t1.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2[i].cpu(),\n",
    "                \"prediction\": x_gen[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "\n",
    "    return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85105d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        super().__init__()\n",
    "        self.directory = directory\n",
    "        self.files = sorted([\n",
    "            f for f in os.listdir(directory) if f.endswith('.pt')\n",
    "        ])\n",
    "        if not self.files:\n",
    "            raise ValueError(f\"No .pt files found in directory: {directory}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.directory, self.files[idx])\n",
    "        data = torch.load(file_path)\n",
    "        pred = data[\"prediction\"]       # expected shape: [1, H, W] or [C, H, W]\n",
    "        gt = data[\"target\"]\n",
    "        return pred, gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a495785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percnorm(arr, lperc=5, uperc=99.5):\n",
    "    \"\"\"\n",
    "    Remove outlier intensities from a brain component,\n",
    "    similar to Tukey's fences method.\n",
    "    \"\"\"\n",
    "    upperbound = np.percentile(arr, uperc)\n",
    "    lowerbound = np.percentile(arr, lperc)\n",
    "    arr[arr > upperbound] = upperbound\n",
    "    arr[arr < lowerbound] = lowerbound\n",
    "    return arr\n",
    "\n",
    "def normalize(img):\n",
    "    # img: [C, H, W]\n",
    "    img = (img - img.min())/ (img.max() - img.min() + 1e-8)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efeb967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssim_from_dataset(dataset):\n",
    "    ssim_scores = []\n",
    "    mse_scores = []\n",
    "\n",
    "    example_pred = None\n",
    "    example_gt = None\n",
    "\n",
    "    crop = transforms.CenterCrop((182, 150))\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        pred, gt = dataset[i]  # tensors [1, H, W]\n",
    "\n",
    "        # Convert to numpy and squeeze channel\n",
    "        pred_np = pred.squeeze().cpu().numpy()\n",
    "        gt_np = gt.squeeze().cpu().numpy()\n",
    "\n",
    "        pred_np = normalize(percnorm(pred_np))\n",
    "        gt_np = normalize(percnorm(gt_np))\n",
    "\n",
    "        # Compute SSIM\n",
    "        ssim_val = ssim_fn(pred_np, gt_np, data_range=1.0)\n",
    "        ssim_scores.append(ssim_val)\n",
    "\n",
    "        # Compute MSE\n",
    "        mse_val = F.mse_loss(pred, gt).item()\n",
    "        mse_scores.append(mse_val)\n",
    "\n",
    "        # Store one example for visualization\n",
    "        if i == 4 and example_pred is None:\n",
    "            example_pred = pred_np\n",
    "            example_gt = gt_np\n",
    "\n",
    "    ssim_scores = np.array(ssim_scores)\n",
    "    mse_scores = np.array(mse_scores)\n",
    "\n",
    "    # Create a DataFrame for SSIM and MSE scores and mean/variance\n",
    "    summary = pd.DataFrame({\n",
    "        \"Metric\": [\"SSIM\", \"MSE\"],\n",
    "        \"Mean\": [ssim_scores.mean(), mse_scores.mean()],\n",
    "        \"Variance\": [ssim_scores.var(), mse_scores.var()]\n",
    "    })\n",
    "\n",
    "    # Visualize example\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axs[0].imshow(example_gt, cmap='gray')\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(example_pred, cmap='gray')\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Example Comparison\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad318d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=2,  #  noise + t1 \n",
    "    out_channels=1  #  predice delta_x_t solo sul noise (condizionato da t1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f910d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154825/88202828.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/andrea_moschetto/flow_matching_t1t2/checkpoints/checkpoint_unetflow-noiset1t2-150e_137.pth', map_location=device)['model_state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/home/andrea_moschetto/flow_matching_t1t2/checkpoints/checkpoint_unetflow-noiset1t2-150e_137.pth', map_location=device)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2e582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250522_140927-f9z9200i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i' target=\"_blank\">finetuning50e-unetflow-noiset1t2-150e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|████████████████████████████████████████████████| 293/293 [03:29<00:00,  1.40it/s]\n",
      "Training epoch 1: 100%|████████████████████████████████████████████████| 293/293 [03:43<00:00,  1.31it/s]\n",
      "Epochs:   4%|██▎                                                       | 2/50 [07:48<3:08:30, 235.63s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_name = \"finetuning50e-unetflow-noiset1t2-150e\"\n",
    "modelpath = train_flow(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader,\n",
    "    project='flowmatching-t1-to-t2', \n",
    "    exp_name=exp_name,\n",
    "    notes=\"UNet flow model for directional diffusion from noise+T1 to T2.\",\n",
    "    n_epochs=50, \n",
    "    lr=1e-7,\n",
    "    generation_steps=300)\n",
    "\n",
    "generate_and_save_predictions(model, test_loader, device,output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f\"evaluation-{exp_name}\",\n",
    "    notes=\"SSIM and MSE scores for the generated predictions.\"\n",
    "):\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    wandb.log({\"ssim-mse\": wandb.Table(dataframe=summary)})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the best checkpoint\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(modelpath, map_location=device)['model_state_dict'])\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
