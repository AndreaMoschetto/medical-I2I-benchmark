{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407529e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim_fn\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fee7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '/home/andrea_moschetto/FlowMatching-MREConversion/data'\n",
    "OUTPUT_DIR = \"/home/andrea_moschetto/FlowMatching-MREConversion/outputs\"\n",
    "CHECKPOINTS_PATH = '/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2823cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedBrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split=\"train\", seed=42):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"split must be 'train', 'val' or 'test'\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.seed = seed\n",
    "        self.samples = self._create_file_pairs()\n",
    "        self._split_dataset()\n",
    "\n",
    "    def _create_file_pairs(self):\n",
    "        t1_dir = os.path.join(self.root_dir, \"t1\")\n",
    "        t2_dir = os.path.join(self.root_dir, \"t2\")\n",
    "\n",
    "        t1_files = set(os.listdir(t1_dir))\n",
    "        t2_files = set(os.listdir(t2_dir))\n",
    "        common_files = list(t1_files.intersection(t2_files))\n",
    "        common_files.sort()\n",
    "\n",
    "        pairs = [(os.path.join(t1_dir, fname), os.path.join(t2_dir, fname)) for fname in common_files]\n",
    "        return pairs\n",
    "\n",
    "    def _split_dataset(self):\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "        n_total = len(self.samples)\n",
    "        n_train = int(n_total * 0.80)\n",
    "        n_val = int(n_total * 0.05)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.samples = self.samples[:n_train]\n",
    "        elif self.split == \"val\":\n",
    "            self.samples = self.samples[n_train:n_train + n_val]\n",
    "        elif self.split == \"test\":\n",
    "            self.samples = self.samples[n_train + n_val:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path, t2_path = self.samples[idx]\n",
    "        t1_image = Image.open(t1_path).convert(\"L\")\n",
    "        t2_image = Image.open(t2_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            t1_image = self.transform(t1_image)\n",
    "            t2_image = self.transform(t2_image)\n",
    "\n",
    "        return {\n",
    "            \"t1\": t1_image,\n",
    "            \"t2\": t2_image,\n",
    "            \"filename\": os.path.basename(t1_path)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab03501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_step(model: DiffusionModelUNet, x_t: Tensor, t_start: Tensor, t_end: Tensor):\n",
    "    # delta_t shape (B, 1, 1, 1)\n",
    "    delta_t = (t_end - t_start).view(-1, 1, 1, 1)\n",
    "    \n",
    "    # model si aspetta t come tensor (B,)\n",
    "    v_hat = model(x_t, t_start)\n",
    "    \n",
    "    x_t_noise = x_t[:, 0:1,:, :] # [B, 1, H, W]\n",
    "    x_t_cond = x_t[:, 1:2, :, :] # [B, 1, H, W], che è T1\n",
    "    \n",
    "    x_next_noise = x_t_noise + delta_t * v_hat\n",
    "    \n",
    "    x_next = torch.cat([x_next_noise, x_t_cond], dim=1) # [B, 2, H, W]\n",
    "    return x_next\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model: nn.Module, x_cond: Tensor, n_steps: int = 20):\n",
    "    model.eval()\n",
    "    \n",
    "    device = x_cond.device\n",
    "    batch_size = x_cond.shape[0]\n",
    "    \n",
    "    time_steps = torch.linspace(0.0, 1.0, n_steps + 1, device=device, dtype=torch.float32)\n",
    "    \n",
    "    x = torch.cat([torch.randn_like(x_cond,device=device), x_cond], dim=1) # [B, 2, H, W]\n",
    "    for i in range(n_steps):\n",
    "        t_start = time_steps[i].expand(batch_size)\n",
    "        t_end = time_steps[i + 1].expand(batch_size)\n",
    "        x = euler_step(model, x_t=x, t_start=t_start, t_end=t_end)\n",
    "    \n",
    "    return x[:, 0:1, :, :] # [B, 1, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e67e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percnorm(arr, lperc=5, uperc=99.5):\n",
    "    \"\"\"\n",
    "    Remove outlier intensities from a brain component,\n",
    "    similar to Tukey's fences method.\n",
    "    \"\"\"\n",
    "    upperbound = np.percentile(arr, uperc)\n",
    "    lowerbound = np.percentile(arr, lperc)\n",
    "    arr[arr > upperbound] = upperbound\n",
    "    arr[arr < lowerbound] = lowerbound\n",
    "    return arr\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    # img: [C, H, W]\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(in_tensor_img):\n",
    "    # tensor_img: [1, H, W]\n",
    "    img_np = in_tensor_img.squeeze(0).cpu().numpy()  # [H, W]\n",
    "    # Percentile-based normalization\n",
    "    img_np = percnorm(img_np)\n",
    "    out_tensor_image = torch.from_numpy(\n",
    "        img_np).unsqueeze(0)  # Back to [1, H, W]\n",
    "    out_tensor_image = normalize(\n",
    "        out_tensor_image)            # 0-1 normalization\n",
    "    return out_tensor_image.to(in_tensor_img.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a03b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_generation(model, epoch, device, n_steps: int, reference_image=None, use_wandb=True):\n",
    "    with torch.no_grad():\n",
    "        if reference_image is not None:\n",
    "            reference_t1, reference_t2 = reference_image  # t1 = input, t2 = ground truth\n",
    "            x_cond = reference_t1.to(device)                 # input al modello = t1\n",
    "            x_gen = generate(model=model, x_cond=x_cond, n_steps=n_steps)\n",
    "\n",
    "            # Visualizza t1 (input), t2 (vero), x_gen (generato)\n",
    "            images = torch.cat([reference_t1, reference_t2, x_gen], dim=0)  # [3, 1, H, W]\n",
    "        else:\n",
    "            raise ValueError(\"reference_image must be provided when generating from t1.\")\n",
    "\n",
    "        grid = torchvision.utils.make_grid(images, nrow=3, normalize=True)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"generation\": [wandb.Image(grid, caption=f\"Epoch {epoch+1}\")]\n",
    "            })\n",
    "\n",
    "        return grid, x_gen\n",
    "\n",
    "\n",
    "\n",
    "def show_grid(grid):\n",
    "    # grid è un tensore [C, H, W], lo trasformiamo in un'immagine visualizzabile\n",
    "    np_grid = grid.permute(1, 2, 0).cpu().numpy()  # da [C,H,W] a [H,W,C]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np_grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debe8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{CHECKPOINTS_PATH}/backups'):\n",
    "    os.makedirs(f'{CHECKPOINTS_PATH}/backups')\n",
    "\n",
    "\n",
    "def train_flow(model: DiffusionModelUNet, device: str, train_loader: DataLoader, val_loader: DataLoader, project: str, exp_name: str, notes: str, n_epochs: int = 10, lr : float = 1e-3, generation_steps: int = 100):\n",
    "    with wandb.init(\n",
    "        project=project,\n",
    "        name=exp_name,\n",
    "        notes=notes,\n",
    "        tags=[\"flow\", \"brain\", \"diffusion\"],\n",
    "        config={\n",
    "            'model': model.__class__.__name__,\n",
    "            'epochs': n_epochs,\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'num_workers': train_loader.num_workers,\n",
    "            'optimizer': 'Adam',\n",
    "            'learning_rate': f'{lr} -> 1e-6',\n",
    "            'loss_function': 'MSELoss',\n",
    "            'generation_steps': generation_steps,\n",
    "            'device': str(torch.cuda.get_device_name(0)\n",
    "                          if torch.cuda.is_available() else \"CPU\"),\n",
    "        }\n",
    "    ) as run:\n",
    "        print(\"Using\", torch.cuda.get_device_name(0)\n",
    "                if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_path = None\n",
    "        start_time = time.time()\n",
    "        for e in trange(n_epochs, desc=\"Epochs\"):\n",
    "            start_e_time = time.time()\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            for batch in tqdm(train_loader, desc=f\"Training epoch {e}\"):\n",
    "                x_1 = batch[\"t2\"].to(device)  # [B, 1, H, W]\n",
    "                x_0_cond = batch[\"t1\"].to(device)  # [B, 1, H, W]  # torch.randn_like(x_1).to(device)  # [B, 1, H, W]\n",
    "                x_0_noise = torch.randn_like(x_0_cond).to(device)  # [B, 1, H, W]\n",
    "                \n",
    "                # add the corresponding t1 to the second channel of x_0\n",
    "                B = x_0_cond.shape[0]\n",
    "                t = torch.rand(B, device=device)  # B \n",
    "                t_img = t.view(B, 1, 1, 1)  # [B, 1, 1, 1] for broadcasting\n",
    "\n",
    "                x_t = (1 - t_img) * x_0_noise + t_img * x_1  # [B, 1, H, W]\n",
    "                x_t = torch.cat([x_t, x_0_cond], dim=1)  # [B, 2, H, W]\n",
    "                \n",
    "                dx_t = x_1 - x_0_noise  # [B, 1, H, W]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(x_t, t)  # [B, 1, H, W]\n",
    "                loss = criterion(pred, dx_t)\n",
    "                train_losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            wandb.log({\"train_loss\": sum(train_losses) / len(train_losses)})\n",
    "            wandb.log({\"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_1 = batch[\"t2\"].to(device)\n",
    "                    x_0_cond = batch[\"t1\"].to(device)\n",
    "                    x_0_noise = torch.randn_like(x_0_cond).to(device)  # [B, 1, H, W]\n",
    "                    B = x_0_cond.shape[0]\n",
    "                    t = torch.rand(B, device=device)\n",
    "                    t_img = t.view(B, 1, 1, 1)\n",
    "                    x_t = (1 - t_img) * x_0_noise + t_img * x_1\n",
    "                    x_t = torch.cat([x_t, x_0_cond], dim=1)  # [B, 2, H, W]\n",
    "                    dx_t = x_1 - x_0_noise\n",
    "\n",
    "                    pred = model(x_t, t)\n",
    "                    val_loss = criterion(pred, dx_t)\n",
    "                    val_losses.append(val_loss.item())\n",
    "            batch_val_loss = sum(val_losses) / len(val_losses)\n",
    "            wandb.log({\"val_loss\": batch_val_loss})\n",
    "            e_time = time.time() - start_e_time\n",
    "            wandb.log({\"epoch_time_minutes\": e_time // 60})\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            # Checkpoint\n",
    "            if e % 5 == 0 or e == n_epochs-1 or batch_val_loss < best_val_loss:\n",
    "                sample_batch = next(iter(val_loader))  # just one batch\n",
    "                t1_gt = sample_batch[\"t1\"][0].unsqueeze(0).to(device)\n",
    "                t2_gt = sample_batch[\"t2\"][0].to(device)  # [1, 1, H, W]\n",
    "                t2_gen = generate(model=model, x_cond=t1_gt, n_steps=generation_steps)  # [1, 1, H, W]\n",
    "                imgs = torch.stack([t1_gt.squeeze(0), t2_gt, preprocess_image(t2_gen.squeeze(0))], dim=0)\n",
    "                grid = torchvision.utils.make_grid(imgs, nrow=3)\n",
    "                \n",
    "                if batch_val_loss < best_val_loss:\n",
    "                    path = f'{CHECKPOINTS_PATH}/checkpoint_{exp_name}_{e+1}_best.pth'\n",
    "                    torch.save({\n",
    "                        'epoch': e+1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, path)\n",
    "                    if best_model_path is not None and os.path.exists(best_model_path):\n",
    "                        os.remove(best_model_path)\n",
    "                    best_model_path = path\n",
    "                    best_val_loss = batch_val_loss\n",
    "                    wandb.log({\n",
    "                        \"best_model_generations\": [wandb.Image(grid, caption=f\"Epoch {e+1} - Best\")]\n",
    "                    })\n",
    "                else:\n",
    "                    path = f'{CHECKPOINTS_PATH}/backups/checkpoint_{exp_name}_{e+1}.pth'\n",
    "                    torch.save({\n",
    "                        'epoch': e+1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, path)\n",
    "                    wandb.log({\n",
    "                        \"each5e_generation\": [wandb.Image(grid, caption=f\"Epoch {e+1}\")]\n",
    "                    })\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        wandb.log({\"total_running_hours\": elapsed_time // 3600})\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "    print(\"Training complete.\")\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8b29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_and_save_predictions(model, test_loader, device, output_dir=OUTPUT_DIR):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        t1 = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2 = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        x_gen = generate(model, x_cond=t1, n_steps=300)\n",
    "        # print(t2.shape, x_gen.shape)\n",
    "\n",
    "        for i in range(t1.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2[i].cpu(),\n",
    "                \"prediction\": x_gen[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "        wandb.log({\"prediction_progress\": idx})\n",
    "\n",
    "    return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85105d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        super().__init__()\n",
    "        self.directory = directory\n",
    "        self.files = sorted([\n",
    "            f for f in os.listdir(directory) if f.endswith('.pt')\n",
    "        ])\n",
    "        if not self.files:\n",
    "            raise ValueError(f\"No .pt files found in directory: {directory}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.directory, self.files[idx])\n",
    "        data = torch.load(file_path)\n",
    "        pred = data[\"prediction\"]       # expected shape: [1, H, W] or [C, H, W]\n",
    "        gt = data[\"target\"]\n",
    "        return pred, gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efeb967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "\n",
    "def compute_ssim_from_dataset(dataset):\n",
    "    ssim_scores = []\n",
    "    mse_scores = []\n",
    "    ssim_masked_scores = []\n",
    "    psnr_scores = []\n",
    "\n",
    "    example_pred = None\n",
    "    example_gt = None\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        pred, gt = dataset[i]  # tensors [1, H, W]\n",
    "\n",
    "        # Convert to numpy and squeeze channel\n",
    "        pred_np = pred.squeeze().cpu().numpy()\n",
    "        gt_np = gt.squeeze().cpu().numpy()\n",
    "\n",
    "        pred_np = normalize(percnorm(pred_np))\n",
    "        gt_np = normalize(percnorm(gt_np))\n",
    "\n",
    "        threshold_triangle_gt = filters.threshold_triangle(gt_np)\n",
    "        mask = gt_np > threshold_triangle_gt\n",
    "        masked_gt = gt_np * mask\n",
    "\n",
    "        threshold_triangle_pred = filters.threshold_triangle(pred_np)\n",
    "        mask_pred = pred_np > threshold_triangle_pred\n",
    "        masked_pred = pred_np * mask_pred\n",
    "\n",
    "        # Compute SSIM\n",
    "        ssim_masked = ssim_fn(masked_gt, masked_pred, data_range=1.0)\n",
    "        ssim_masked_scores.append(ssim_masked)\n",
    "\n",
    "        ssim_val = ssim_fn(pred_np, gt_np, data_range=1.0)\n",
    "        ssim_scores.append(ssim_val)\n",
    "\n",
    "        # Compute MSE\n",
    "        mse_val = F.mse_loss(pred, gt).item()\n",
    "        mse_scores.append(mse_val)\n",
    "        \n",
    "        # Compute PSNR on masked images\n",
    "        psnr_val = psnr_fn(masked_gt, masked_pred, data_range=1.0)\n",
    "        psnr_scores.append(psnr_val)\n",
    "        \n",
    "\n",
    "        # Store one example for visualization\n",
    "        if i == 4 and example_pred is None:\n",
    "            example_pred = masked_pred\n",
    "            example_gt = masked_gt\n",
    "\n",
    "    ssim_scores = np.array(ssim_scores)\n",
    "    ssim_masked_scores = np.array(ssim_masked_scores)\n",
    "    mse_scores = np.array(mse_scores)\n",
    "    psnr_scores = np.array(psnr_scores)\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"Metric\": [\"SSIM\", \"MASKED_SSIM\", \"MSE\", \"PSNR\"],\n",
    "        \"Mean\": [ssim_scores.mean(), ssim_masked_scores.mean(), mse_scores.mean(), psnr_scores.mean()],\n",
    "        \"Variance\": [ssim_scores.var(), ssim_masked_scores.var(), mse_scores.var(), psnr_scores.var()],\n",
    "    })\n",
    "\n",
    "    # Visualize example\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axs[0].imshow(example_gt, cmap='gray')\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(example_pred, cmap='gray')\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Example Comparison\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad318d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=2,  #  noise + t1 \n",
    "    out_channels=1  #  predice delta_x_t solo sul noise (condizionato da t1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f910d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5372/3463383609.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-noiset1t2-150e_137.pth', map_location=device)['model_state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-noiset1t2-150e_137.pth', map_location=device)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2e582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250522_140927-f9z9200i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i' target=\"_blank\">finetuning50e-unetflow-noiset1t2-150e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/f9z9200i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|████████████████████████████████████████████████| 293/293 [03:29<00:00,  1.40it/s]\n",
      "Training epoch 1: 100%|████████████████████████████████████████████████| 293/293 [03:43<00:00,  1.31it/s]\n",
      "Epochs:   4%|██▎                                                       | 2/50 [07:48<3:08:30, 235.63s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_name = \"finetuning50e-unetflow-noiset1t2-150e\"\n",
    "modelpath = train_flow(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader,\n",
    "    project='flowmatching-t1-to-t2', \n",
    "    exp_name=exp_name,\n",
    "    notes=\"UNet flow model for directional diffusion from noise+T1 to T2.\",\n",
    "    n_epochs=50, \n",
    "    lr=1e-7,\n",
    "    generation_steps=300)\n",
    "\n",
    "generate_and_save_predictions(model, test_loader, device,output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f\"evaluation-{exp_name}\",\n",
    "    notes=\"SSIM and MSE scores for the generated predictions.\"\n",
    "):\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    wandb.log({\"ssim-mse\": wandb.Table(dataframe=summary)})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the best checkpoint\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(modelpath, map_location=device)['model_state_dict'])\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250609_103742-er6704s0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/er6704s0' target=\"_blank\">evaluation-unetflow-noiset1t2-150e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/er6704s0' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/er6704s0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions:  47%|████████████████████▊                       | 26/55 [32:47<36:47, 76.14s/it]"
     ]
    }
   ],
   "source": [
    "exp_name = \"unetflow-noiset1t2-150e\"\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f\"evaluation-{exp_name}\",\n",
    "    notes=\"SSIM and MSE scores for the generated predictions.\"\n",
    "):\n",
    "    generate_and_save_predictions(model, test_loader, device,output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    wandb.log({\"eval/metrics\": wandb.Table(dataframe=summary)}) # log the summary table\n",
    "    wandb.log({\"eval/ssim_mean\": summary[\"Mean\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_mean\": summary[\"Mean\"][1]})\n",
    "    wandb.log({\"eval/mse_mean\": summary[\"Mean\"][2]})\n",
    "    wandb.log({\"eval/ssim_var\": summary[\"Variance\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_var\": summary[\"Variance\"][1]})\n",
    "    wandb.log({\"eval/mse_var\": summary[\"Variance\"][2]})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e774f34",
   "metadata": {},
   "source": [
    "## Training with a Bigger Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332221b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=2,  # x+noise\n",
    "    out_channels=1,  # predice delta_x_t\n",
    "    num_channels=(64, 128, 256, 256),\n",
    "    attention_levels=(False, False, True, True),\n",
    "    num_head_channels=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a00998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandreamoschetto99\u001b[0m (\u001b[33mandreamoschetto99-university-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250617_013145-6fu4qc9m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/6fu4qc9m' target=\"_blank\">big-unetflow-noiset1t2-s300e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/6fu4qc9m' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/6fu4qc9m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                                                    | 0/300 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "exp_name = \"big-unetflow-noiset1t2-s300e\"\n",
    "best_modelpath = train_flow(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    exp_name=exp_name,\n",
    "    notes=\"Big UNet flow model for directional diffusion from T1+Noise to T2. 300 epochs.\",\n",
    "    n_epochs=300,\n",
    "    lr=5e-4,\n",
    "    generation_steps=300)\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load(best_modelpath, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f'evaluation-{exp_name}',\n",
    "    notes=\"Evaluation of the flow model on the test set.\",\n",
    "):\n",
    "    generate_and_save_predictions(model, test_loader, device, output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    # log the summary table\n",
    "    wandb.log({\"eval/ssim_mean\": summary[\"Mean\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_mean\": summary[\"Mean\"][1]})\n",
    "    wandb.log({\"eval/mse_mean\": summary[\"Mean\"][2]})\n",
    "    wandb.log({\"eval/psnr_mean\": summary[\"Mean\"][3]})\n",
    "    wandb.log({\"eval/ssim_var\": summary[\"Variance\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_var\": summary[\"Variance\"][1]})\n",
    "    wandb.log({\"eval/mse_var\": summary[\"Variance\"][2]})\n",
    "    wandb.log({\"eval/psnr_var\": summary[\"Variance\"][3]})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a4fc3",
   "metadata": {},
   "source": [
    "## Retrain a small Unet with scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701939d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers=2, shuffle=True)\n",
    "val_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=2,  # x+noise\n",
    "    out_channels=1,  # predice delta_x_t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adddbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 48: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.30it/s]-of-catania\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Training epoch 49: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.29it/s]\n",
      "Training epoch 50: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 51: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 52: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 53: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 54: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 55: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 56: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 57: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 58: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 59: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 60: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 61: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 62: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 63: 100%|███████████████████████████████████████████████| 293/293 [03:46<00:00,  1.30it/s]\n",
      "Training epoch 64: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 65: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 66: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 67: 100%|███████████████████████████████████████████████| 293/293 [03:45<00:00,  1.30it/s]\n",
      "Training epoch 122: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 123: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 124: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 125: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 126: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 127: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 128: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 129: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 130: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 131: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 132: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 133: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 134: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 135: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 136: 100%|██████████████████████████████████████████████| 293/293 [03:41<00:00,  1.32it/s]\n",
      "Training epoch 137: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 138: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 139: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 140: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 141: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 142: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 143: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 144: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 145: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 146: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 147: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 148: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 149: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 150: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 151: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 152: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 153: 100%|██████████████████████████████████████████████| 293/293 [03:42<00:00,  1.32it/s]\n",
      "Training epoch 154: 100%|██████████████████████████████████████████████| 293/293 [03:41<00:00,  1.32it/s]\n",
      "Epochs:  52%|██████████████████████████▊                         | 155/300 [10:00:40<9:11:12, 228.09s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrea_moschetto/wandb/run-20250619_143901-7mkr41o0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/7mkr41o0' target=\"_blank\">unetflow-noiset1t2-s300e</a></strong> to <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/7mkr41o0' target=\"_blank\">https://wandb.ai/andreamoschetto99-university-of-catania/flowmatching-t1-to-t2/runs/7mkr41o0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                                                    | 0/300 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "exp_name = \"unetflow-noiset1t2-s300e\"\n",
    "best_modelpath = train_flow(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    exp_name=exp_name,\n",
    "    notes=\"Big UNet flow model for directional diffusion from T1+Noise to T2. 300 epochs.\",\n",
    "    n_epochs=300,\n",
    "    lr=3e-4,\n",
    "    generation_steps=300)\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load(best_modelpath, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "with wandb.init(\n",
    "    project='flowmatching-t1-to-t2',\n",
    "    name=f'evaluation-{exp_name}',\n",
    "    notes=\"Evaluation of the flow model on the test set.\",\n",
    "):\n",
    "    generate_and_save_predictions(model, test_loader, device, output_dir=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    out_dataset = PredictionDataset(directory=f'{OUTPUT_DIR}/{exp_name}')\n",
    "    summary = compute_ssim_from_dataset(out_dataset)\n",
    "    # log the summary table\n",
    "    wandb.log({\"eval/ssim_mean\": summary[\"Mean\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_mean\": summary[\"Mean\"][1]})\n",
    "    wandb.log({\"eval/mse_mean\": summary[\"Mean\"][2]})\n",
    "    wandb.log({\"eval/psnr_mean\": summary[\"Mean\"][3]})\n",
    "    wandb.log({\"eval/ssim_var\": summary[\"Variance\"][0]})\n",
    "    wandb.log({\"eval/ssim_masked_var\": summary[\"Variance\"][1]})\n",
    "    wandb.log({\"eval/mse_var\": summary[\"Variance\"][2]})\n",
    "    wandb.log({\"eval/psnr_var\": summary[\"Variance\"][3]})\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
