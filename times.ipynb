{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3beec608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/andrea_moschetto/miniconda3/envs/medical/lib/python3.11/site-packages/generative/networks/layers/vector_quantizer.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 2.0.1\n",
      "Pytorch version: 2.5.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: /home/<username>/miniconda3/envs/medical/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: 0.25.0\n",
      "scipy version: 1.15.3\n",
      "Pillow version: 11.1.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.20.1\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from generative.inferers import DiffusionInferer, ControlNetDiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet, ControlNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from skimage.metrics import structural_similarity as ssim_fn\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_fn\n",
    "import wandb\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cc7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative.networks.nets import DiffusionModelUNet, ControlNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a89c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = DATAPATH = '/home/andrea_moschetto/FlowMatching-MREConversion/data'\n",
    "OUTPUT_DIR = \"/home/andrea_moschetto/FlowMatching-MREConversion/outputs\"\n",
    "CHECKPOINTS_PATH = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427e42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedBrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split=\"train\", seed=42):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"split must be 'train', 'val' or 'test'\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.seed = seed\n",
    "        self.samples = self._create_file_pairs()\n",
    "        self._split_dataset()\n",
    "\n",
    "    def _create_file_pairs(self):\n",
    "        t1_dir = os.path.join(self.root_dir, \"t1\")\n",
    "        t2_dir = os.path.join(self.root_dir, \"t2\")\n",
    "\n",
    "        t1_files = set(os.listdir(t1_dir))\n",
    "        t2_files = set(os.listdir(t2_dir))\n",
    "        common_files = list(t1_files.intersection(t2_files))\n",
    "        common_files.sort()\n",
    "\n",
    "        pairs = [(os.path.join(t1_dir, fname), os.path.join(t2_dir, fname)) for fname in common_files]\n",
    "        return pairs\n",
    "\n",
    "    def _split_dataset(self):\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "        n_total = len(self.samples)\n",
    "        n_train = int(n_total * 0.80)\n",
    "        n_val = int(n_total * 0.05)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.samples = self.samples[:n_train]\n",
    "        elif self.split == \"val\":\n",
    "            self.samples = self.samples[n_train:n_train + n_val]\n",
    "        elif self.split == \"test\":\n",
    "            self.samples = self.samples[n_train + n_val:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path, t2_path = self.samples[idx]\n",
    "        t1_image = Image.open(t1_path).convert(\"L\")\n",
    "        t2_image = Image.open(t2_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            t1_image = self.transform(t1_image)\n",
    "            t2_image = self.transform(t2_image)\n",
    "\n",
    "        return {\n",
    "            \"t1\": t1_image,\n",
    "            \"t2\": t2_image,\n",
    "            \"filename\": os.path.basename(t1_path)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a38228a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fm_direct = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=1,  # x\n",
    "    out_channels=1  # predice delta_x_t\n",
    ")\n",
    "fm_direct = fm_direct.to(device)\n",
    "fm_noise = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=2,  #  noise + t1 \n",
    "    out_channels=1  #  predice delta_x_t solo sul noise (condizionato da t1)\n",
    ")\n",
    "fm_noise = fm_noise.to(device)\n",
    "control_diff = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1\n",
    ")\n",
    "control_diff = control_diff.to(device)\n",
    "controlnet = ControlNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    conditioning_embedding_num_channels=(32, )\n",
    ")\n",
    "controlnet = controlnet.to(device)\n",
    "dm = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=2,\n",
    "    out_channels=1\n",
    ")\n",
    "dm = dm.to(device)\n",
    "pixgen = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1\n",
    ")\n",
    "pixgen = pixgen.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9083275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlnet\n",
    "controldiff_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_diffusion-t2-brain300e_164_best.pth'\n",
    "controlnet_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_controlnet-t1t2-brain300e_164_best.pth'\n",
    "fm_noise_path = '/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-noiset1t2-s300e_46_best.pth'\n",
    "fm_direct_path = '/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-t1t2-s300e_122_best.pth'\n",
    "pixgen_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_unet2pix-t1t2-brain300e_142__generator_best.pth'\n",
    "diffusion_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_diffusion-t1t2-brains300e_200_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fff7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flow matching noise\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def noise_euler_step(model: DiffusionModelUNet, x_t: Tensor, t_start: Tensor, t_end: Tensor):\n",
    "    # delta_t shape (B, 1, 1, 1)\n",
    "    delta_t = (t_end - t_start).view(-1, 1, 1, 1)\n",
    "    \n",
    "    # model si aspetta t come tensor (B,)\n",
    "    v_hat = model(x_t, t_start)\n",
    "    \n",
    "    x_t_noise = x_t[:, 0:1,:, :] # [B, 1, H, W]\n",
    "    x_t_cond = x_t[:, 1:2, :, :] # [B, 1, H, W], che è T1\n",
    "    \n",
    "    x_next_noise = x_t_noise + delta_t * v_hat\n",
    "    \n",
    "    x_next = torch.cat([x_next_noise, x_t_cond], dim=1) # [B, 2, H, W]\n",
    "    return x_next\n",
    "\n",
    "@torch.no_grad()\n",
    "def noise_generate(model: nn.Module, x_cond: Tensor, n_steps: int = 20):\n",
    "    model.eval()\n",
    "    \n",
    "    device = x_cond.device\n",
    "    batch_size = x_cond.shape[0]\n",
    "    \n",
    "    time_steps = torch.linspace(0.0, 1.0, n_steps + 1, device=device, dtype=torch.float32)\n",
    "    \n",
    "    x = torch.cat([torch.randn_like(x_cond,device=device), x_cond], dim=1) # [B, 2, H, W]\n",
    "    for i in range(n_steps):\n",
    "        t_start = time_steps[i].expand(batch_size)\n",
    "        t_end = time_steps[i + 1].expand(batch_size)\n",
    "        x = noise_euler_step(model, x_t=x, t_start=t_start, t_end=t_end)\n",
    "    \n",
    "    return x[:, 0:1, :, :] # [B, 1, H, W]\n",
    "\n",
    "# flow matching direct\n",
    "def direct_euler_step(model: DiffusionModelUNet, x_t: Tensor, t_start: Tensor, t_end: Tensor):\n",
    "    # delta_t shape (B, 1, 1, 1)\n",
    "    delta_t = (t_end - t_start).view(-1, 1, 1, 1)\n",
    "\n",
    "    # model si aspetta t come tensor (B,)\n",
    "    v_hat = model(x_t, t_start)\n",
    "\n",
    "    x_next = x_t + delta_t * v_hat\n",
    "\n",
    "    return x_next\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def direct_generate(model: nn.Module, x_T: Tensor, n_steps: int = 20):\n",
    "    model.eval()\n",
    "\n",
    "    device = x_T.device\n",
    "    batch_size = x_T.shape[0]\n",
    "\n",
    "    time_steps = torch.linspace(\n",
    "        0.0, 1.0, n_steps + 1, device=device, dtype=torch.float32)\n",
    "\n",
    "    x = x_T\n",
    "    for i in range(n_steps):\n",
    "        t_start = time_steps[i].expand(batch_size)\n",
    "        t_end = time_steps[i + 1].expand(batch_size)\n",
    "        x = direct_euler_step(model, x_t=x, t_start=t_start, t_end=t_end)\n",
    "\n",
    "    return x\n",
    "\n",
    "# controlnet\n",
    "    # on inferer\n",
    "\n",
    "#pix2pix\n",
    "    # direct generation\n",
    "\n",
    "#diffusion\n",
    "    # on inferer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6ec5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control net\n",
    "\n",
    "@torch.no_grad()\n",
    "def control_generate_and_save_predictions(cn_model: ControlNet, df_model: DiffusionModelUNet, inferer: ControlNetDiffusionInferer, test_loader: DataLoader, device: str, output_dir: str = OUTPUT_DIR, just_one_batch: bool = False):\n",
    "    start = time.time()\n",
    "    df_model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "    imgsperbatch = None\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        imgsperbatch = len(batch[\"t1\"])\n",
    "        t1_cond = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2_targets = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        noise = torch.randn_like(t2_targets).to(device)          # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "        \n",
    "        gen_images = inferer.sample(input_noise=noise, diffusion_model=df_model, controlnet=cn_model, scheduler=inferer.scheduler, cn_cond=t1_cond, verbose=False)\n",
    "\n",
    "            \n",
    "        if just_one_batch:\n",
    "            break\n",
    "\n",
    "    end = time.time()\n",
    "    # Calculate average time per img on a single batch\n",
    "    avg = (end - start) / imgsperbatch\n",
    "    return avg\n",
    "\n",
    "# pix2pix\n",
    "@torch.no_grad()\n",
    "def pix_generate_and_save_predictions(generator: DiffusionModelUNet, test_loader: DataLoader, device: str, output_dir: str = OUTPUT_DIR, just_one_batch: bool = False):\n",
    "    start = time.time()\n",
    "    generator.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "    imgsperbatch = None\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        imgsperbatch = len(batch[\"t1\"])\n",
    "        with torch.no_grad():\n",
    "            real_A = batch[\"t1\"].to(device)  # [B, 1, H, W]\n",
    "            real_B = batch[\"t2\"].to(device)  # [B, 1, H, W]\n",
    "            filenames = batch[\"filename\"]  # list of strings (length B)\n",
    "            # Generate fake T2 images\n",
    "\n",
    "            gen_images = generator(x=real_A, timesteps=torch.zeros(real_A.shape[0], device=device))\n",
    "            \n",
    "        if just_one_batch:\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    avg = (end - start) / imgsperbatch\n",
    "    return avg\n",
    "\n",
    "# diffusion model\n",
    "@torch.no_grad()\n",
    "def diff_generate_and_save_predictions(model: nn.Module, inferer: DiffusionInferer, test_loader: DataLoader, device: str, output_dir: str = OUTPUT_DIR, just_one_batch: bool = False):\n",
    "    start = time.time()\n",
    "    imgsperbatch = None\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        imgsperbatch = len(batch[\"t1\"])\n",
    "        t1_cond = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2_target = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        noise = torch.randn_like(t2_target).to(device)          # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        gen_image = inferer.sample(input_noise=noise, diffusion_model=model,\n",
    "                                   scheduler=inferer.scheduler, mode='concat', conditioning=t1_cond)\n",
    "\n",
    "        if just_one_batch:\n",
    "            break\n",
    "\n",
    "    end = time.time()\n",
    "    avg = (end - start) / imgsperbatch\n",
    "    return avg\n",
    "\n",
    "# flow matching direct\n",
    "@torch.no_grad()\n",
    "def direct_generate_and_save_predictions(model, test_loader, device, output_dir=OUTPUT_DIR, just_one_batch=False):\n",
    "    start = time.time()\n",
    "    imgsperbatch = None\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "        imgsperbatch = len(batch[\"t1\"])\n",
    "        t1 = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2 = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        x_gen = direct_generate(model, x_T=t1, n_steps=300)\n",
    "\n",
    "        if just_one_batch:\n",
    "            break\n",
    "\n",
    "    end = time.time()\n",
    "    avg = (end - start) / imgsperbatch\n",
    "    return avg\n",
    "\n",
    "#flow matching noise\n",
    "@torch.no_grad()\n",
    "def noise_generate_and_save_predictions(model, test_loader, device, output_dir=OUTPUT_DIR, just_one_batch=False):\n",
    "    start = time.time()\n",
    "    imgsperbatch = None\n",
    "    model.eval()\n",
    "    \n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        imgsperbatch = len(batch[\"t1\"])\n",
    "        t1 = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2 = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        x_gen = noise_generate(model, x_cond=t1, n_steps=300)\n",
    "        # print(t2.shape, x_gen.shape)\n",
    "\n",
    "        if just_one_batch:\n",
    "            break\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    avg = (end - start) / imgsperbatch\n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5e745bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8902/3928744658.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fm_direct_path, map_location=device)\n",
      "/tmp/ipykernel_8902/3928744658.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fm_noise_path, map_location=device)\n",
      "/tmp/ipykernel_8902/3928744658.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(controldiff_path, map_location=device)\n",
      "/tmp/ipykernel_8902/3928744658.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(controlnet_path, map_location=device)\n",
      "/tmp/ipykernel_8902/3928744658.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pixgen_path, map_location=device)\n",
      "/tmp/ipykernel_8902/3928744658.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(diffusion_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(fm_direct_path, map_location=device)\n",
    "fm_direct.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint = torch.load(fm_noise_path, map_location=device)\n",
    "fm_noise.load_state_dict(checkpoint['model_state_dict'])\n",
    "# --------------------------------------\n",
    "num_train_timesteps = 1000\n",
    "checkpoint = torch.load(controldiff_path, map_location=device)\n",
    "control_diff.load_state_dict(checkpoint['model_state_dict'])\n",
    "control_scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps)\n",
    "control_diff_inferer = DiffusionInferer(control_scheduler)\n",
    "\n",
    "checkpoint = torch.load(controlnet_path, map_location=device)\n",
    "controlnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "controlnet_inferer = ControlNetDiffusionInferer(control_scheduler)\n",
    "# --------------------------------------\n",
    "\n",
    "checkpoint = torch.load(pixgen_path, map_location=device)\n",
    "pixgen.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint = torch.load(diffusion_path, map_location=device)\n",
    "dm.load_state_dict(checkpoint['model_state_dict'])\n",
    "dm_scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps)\n",
    "dm_inferer = DiffusionInferer(dm_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04112ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb2071cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions:   0%|                                                     | 0/55 [01:09<?, ?it/s]\n",
      "Generating Predictions:   0%|                                                     | 0/55 [01:11<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 1000/1000 [04:07<00:00,  4.04it/s]\n",
      "Generating Predictions:   0%|                                                     | 0/55 [04:07<?, ?it/s]\n",
      "Generating Predictions:   0%|                                                     | 0/55 [00:00<?, ?it/s]\n",
      "Generating Predictions:   0%|                                                     | 0/55 [05:47<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#direct\n",
    "\n",
    "direct_avg = direct_generate_and_save_predictions(\n",
    "    model=fm_direct,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    output_dir=None,\n",
    "    just_one_batch=True\n",
    ")\n",
    "\n",
    "noise_avg = noise_generate_and_save_predictions(\n",
    "    model=fm_noise,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    output_dir=None,\n",
    "    just_one_batch=True\n",
    ")\n",
    "\n",
    "diff_avg = diff_generate_and_save_predictions(\n",
    "    model=dm,\n",
    "    inferer=dm_inferer,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    output_dir=None,\n",
    "    just_one_batch=True\n",
    ")\n",
    "\n",
    "pix_avg = pix_generate_and_save_predictions(\n",
    "    generator=pixgen,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    output_dir=None,\n",
    "    just_one_batch=True\n",
    ")\n",
    "\n",
    "control_avg = control_generate_and_save_predictions(\n",
    "    cn_model=controlnet,\n",
    "    df_model=control_diff,\n",
    "    inferer=controlnet_inferer,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    output_dir=None,\n",
    "    just_one_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abe67cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model        |  Avg Time (s/img)\n",
      "--------------------------------\n",
      "FM_Direct    |           11.6146\n",
      "FM_Noise     |           11.9371\n",
      "Diffusion    |           41.3273\n",
      "Pix2Pix      |            0.0539\n",
      "ControlNet   |           57.8672\n"
     ]
    }
   ],
   "source": [
    "# Raccogliamo i risultati\n",
    "results = {\n",
    "    \"FM_Direct\": direct_avg,\n",
    "    \"FM_Noise\": noise_avg,\n",
    "    \"Diffusion\": diff_avg,\n",
    "    \"Pix2Pix\": pix_avg,\n",
    "    \"ControlNet\": control_avg\n",
    "}\n",
    "\n",
    "# Stampa tabellare\n",
    "print(f\"{'Model':<12} | {'Avg Time (s/img)':>17}\")\n",
    "print('-' * 32)\n",
    "for name, avg_time in results.items():\n",
    "    print(f\"{name:<12} | {avg_time:>17.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dab3e0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary (parameters and size in MB):\n",
      "\n",
      "ControlDiffusion: 2,328,449 params, 8.88 MB\n",
      "ControlNet  : 927,296 params, 3.54 MB\n",
      "FM_Noise    : 2,328,737 params, 8.88 MB\n",
      "FM_Direct   : 2,328,449 params, 8.88 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8902/33899488.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pix2Pix     : 2,328,449 params, 8.88 MB\n",
      "Diffusion   : 2,328,737 params, 8.88 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_model_state_dict_size(path, key='model_state_dict'):\n",
    "    \"\"\"\n",
    "    Calcola la memoria occupata da un state_dict PyTorch salvato su disco.\n",
    "\n",
    "    Args:\n",
    "        path (str): Percorso del file contenente il checkpoint.\n",
    "        key (str): Chiave del dizionario che contiene il state_dict. Default: 'model_state_dict'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (numero totale di parametri, dimensione in MB)\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "    # Se è un dizionario e contiene il key\n",
    "    if isinstance(checkpoint, dict) and key in checkpoint:\n",
    "        state_dict = checkpoint[key]\n",
    "    else:\n",
    "        # Altrimenti assume che sia direttamente lo state_dict\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    total_params = 0\n",
    "    total_size_bytes = 0\n",
    "\n",
    "    for tensor in state_dict.values():\n",
    "        total_params += tensor.numel()\n",
    "        total_size_bytes += tensor.numel() * tensor.element_size()\n",
    "\n",
    "    total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "\n",
    "    return total_params, total_size_mb\n",
    "\n",
    "\n",
    "# Paths dei modelli\n",
    "paths = {\n",
    "    \"ControlDiffusion\": \"/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_diffusion-t2-brain300e_164_best.pth\",\n",
    "    \"ControlNet\": \"/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_controlnet-t1t2-brain300e_164_best.pth\",\n",
    "    \"FM_Noise\": \"/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-noiset1t2-s300e_46_best.pth\",\n",
    "    \"FM_Direct\": \"/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-t1t2-s300e_122_best.pth\",\n",
    "    \"Pix2Pix\": \"/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_unet2pix-t1t2-brain300e_142__generator_best.pth\",\n",
    "    \"Diffusion\": \"/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_diffusion-t1t2-brains300e_200_best.pth\"\n",
    "}\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(\"Model Summary (parameters and size in MB):\\n\")\n",
    "for name, path in paths.items():\n",
    "    try:\n",
    "        params, size_mb = get_model_state_dict_size(path)\n",
    "        print(f\"{name:<12}: {params:,} params, {size_mb:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:<12}: Error reading checkpoint → {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
