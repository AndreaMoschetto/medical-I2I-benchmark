{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beec608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from generative.inferers import DiffusionInferer, ControlNetDiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet, ControlNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from skimage.metrics import structural_similarity as ssim_fn\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_fn\n",
    "import wandb\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative.networks.nets import DiffusionModelUNet, ControlNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a89c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = DATAPATH = '/home/andrea_moschetto/FlowMatching-MREConversion/data'\n",
    "OUTPUT_DIR = \"/home/andrea_moschetto/FlowMatching-MREConversion/outputs\"\n",
    "CHECKPOINTS_PATH = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedBrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split=\"train\", seed=42):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"split must be 'train', 'val' or 'test'\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.seed = seed\n",
    "        self.samples = self._create_file_pairs()\n",
    "        self._split_dataset()\n",
    "\n",
    "    def _create_file_pairs(self):\n",
    "        t1_dir = os.path.join(self.root_dir, \"t1\")\n",
    "        t2_dir = os.path.join(self.root_dir, \"t2\")\n",
    "\n",
    "        t1_files = set(os.listdir(t1_dir))\n",
    "        t2_files = set(os.listdir(t2_dir))\n",
    "        common_files = list(t1_files.intersection(t2_files))\n",
    "        common_files.sort()\n",
    "\n",
    "        pairs = [(os.path.join(t1_dir, fname), os.path.join(t2_dir, fname)) for fname in common_files]\n",
    "        return pairs\n",
    "\n",
    "    def _split_dataset(self):\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "        n_total = len(self.samples)\n",
    "        n_train = int(n_total * 0.80)\n",
    "        n_val = int(n_total * 0.05)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.samples = self.samples[:n_train]\n",
    "        elif self.split == \"val\":\n",
    "            self.samples = self.samples[n_train:n_train + n_val]\n",
    "        elif self.split == \"test\":\n",
    "            self.samples = self.samples[n_train + n_val:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path, t2_path = self.samples[idx]\n",
    "        t1_image = Image.open(t1_path).convert(\"L\")\n",
    "        t2_image = Image.open(t2_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            t1_image = self.transform(t1_image)\n",
    "            t2_image = self.transform(t2_image)\n",
    "\n",
    "        return {\n",
    "            \"t1\": t1_image,\n",
    "            \"t2\": t2_image,\n",
    "            \"filename\": os.path.basename(t1_path)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38228a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_direct = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=1,  # x\n",
    "    out_channels=1  # predice delta_x_t\n",
    ")\n",
    "fm_noise = DiffusionModelUNet(\n",
    "    spatial_dims=2,  #  2D\n",
    "    in_channels=2,  #  noise + t1 \n",
    "    out_channels=1  #  predice delta_x_t solo sul noise (condizionato da t1)\n",
    ")\n",
    "control_diff = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1\n",
    ")\n",
    "controlnet = ControlNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    conditioning_embedding_num_channels=(32, )\n",
    ")\n",
    "dm = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=2,\n",
    "    out_channels=1\n",
    ")\n",
    "pixgen = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9083275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlnet\n",
    "controldiff_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_diffusion-t2-brain300e_164_best.pth'\n",
    "controlnet_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_controlnet-t1t2-brain300e_164_best.pth'\n",
    "fm_noise_path = '/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-noiset1t2-s300e_46_best.pth'\n",
    "fm_direct_path = '/home/andrea_moschetto/FlowMatching-MREConversion/checkpoints/checkpoint_unetflow-t1t2-s300e_122_best.pth'\n",
    "pixgen_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_pix2pix-t1t2-brain300e_169__generator_best.pth'\n",
    "diffusion_path = '/home/andrea_moschetto/FlowMatching-MREConversion/baseline_checkpoints/checkpoint_diffusion-t2-brain300e_164_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flow matching noise\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def noise_euler_step(model: DiffusionModelUNet, x_t: Tensor, t_start: Tensor, t_end: Tensor):\n",
    "    # delta_t shape (B, 1, 1, 1)\n",
    "    delta_t = (t_end - t_start).view(-1, 1, 1, 1)\n",
    "    \n",
    "    # model si aspetta t come tensor (B,)\n",
    "    v_hat = model(x_t, t_start)\n",
    "    \n",
    "    x_t_noise = x_t[:, 0:1,:, :] # [B, 1, H, W]\n",
    "    x_t_cond = x_t[:, 1:2, :, :] # [B, 1, H, W], che è T1\n",
    "    \n",
    "    x_next_noise = x_t_noise + delta_t * v_hat\n",
    "    \n",
    "    x_next = torch.cat([x_next_noise, x_t_cond], dim=1) # [B, 2, H, W]\n",
    "    return x_next\n",
    "\n",
    "@torch.no_grad()\n",
    "def noise_generate(model: nn.Module, x_cond: Tensor, n_steps: int = 20):\n",
    "    model.eval()\n",
    "    \n",
    "    device = x_cond.device\n",
    "    batch_size = x_cond.shape[0]\n",
    "    \n",
    "    time_steps = torch.linspace(0.0, 1.0, n_steps + 1, device=device, dtype=torch.float32)\n",
    "    \n",
    "    x = torch.cat([torch.randn_like(x_cond,device=device), x_cond], dim=1) # [B, 2, H, W]\n",
    "    for i in range(n_steps):\n",
    "        t_start = time_steps[i].expand(batch_size)\n",
    "        t_end = time_steps[i + 1].expand(batch_size)\n",
    "        x = noise_euler_step(model, x_t=x, t_start=t_start, t_end=t_end)\n",
    "    \n",
    "    return x[:, 0:1, :, :] # [B, 1, H, W]\n",
    "\n",
    "# flow matching direct\n",
    "def direct_euler_step(model: DiffusionModelUNet, x_t: Tensor, t_start: Tensor, t_end: Tensor):\n",
    "    # delta_t shape (B, 1, 1, 1)\n",
    "    delta_t = (t_end - t_start).view(-1, 1, 1, 1)\n",
    "\n",
    "    # model si aspetta t come tensor (B,)\n",
    "    v_hat = model(x_t, t_start)\n",
    "\n",
    "    x_next = x_t + delta_t * v_hat\n",
    "\n",
    "    return x_next\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def direct_generate(model: nn.Module, x_T: Tensor, n_steps: int = 20):\n",
    "    model.eval()\n",
    "\n",
    "    device = x_T.device\n",
    "    batch_size = x_T.shape[0]\n",
    "\n",
    "    time_steps = torch.linspace(\n",
    "        0.0, 1.0, n_steps + 1, device=device, dtype=torch.float32)\n",
    "\n",
    "    x = x_T\n",
    "    for i in range(n_steps):\n",
    "        t_start = time_steps[i].expand(batch_size)\n",
    "        t_end = time_steps[i + 1].expand(batch_size)\n",
    "        x = direct_euler_step(model, x_t=x, t_start=t_start, t_end=t_end)\n",
    "\n",
    "    return x\n",
    "\n",
    "# controlnet\n",
    "    # on inferer\n",
    "\n",
    "#pix2pix\n",
    "    # direct generation\n",
    "\n",
    "#diffusion\n",
    "    # on inferer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control net\n",
    "\n",
    "@torch.no_grad()\n",
    "def control_generate_and_save_predictions(cn_model: ControlNet, df_model: DiffusionModelUNet, inferer: ControlNetDiffusionInferer, test_loader: DataLoader, device: str, output_dir: str = OUTPUT_DIR, just_one_batch: bool = False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        with autocast(device_type=\"cuda\", enabled=True):\n",
    "            t1_cond = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "            t2_targets = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "            noise = torch.randn_like(t2_targets).to(device)          # [B, 1, H, W]\n",
    "            filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "            \n",
    "            gen_images = inferer.sample(input_noise=noise, diffusion_model=df_model, controlnet=cn_model, scheduler=inferer.scheduler, cn_cond=t1_cond, verbose=False)\n",
    "\n",
    "        for i in range(t1_cond.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1_cond[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2_targets[i].cpu(),\n",
    "                \"prediction\": gen_images[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "            \n",
    "        if just_one_batch:\n",
    "            break\n",
    "        wandb.log({\"prediction_progress\": idx})\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "# pix2pix\n",
    "@torch.no_grad()\n",
    "def pix_generate_and_save_predictions(generator: DiffusionModelUNet, test_loader: DataLoader, device: str, output_dir: str = OUTPUT_DIR, just_one_batch: bool = False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    generator.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        with torch.no_grad():\n",
    "            real_A = batch[\"t1\"].to(device)  # [B, 1, H, W]\n",
    "            real_B = batch[\"t2\"].to(device)  # [B, 1, H, W]\n",
    "            filenames = batch[\"filename\"]  # list of strings (length B)\n",
    "            # Generate fake T2 images\n",
    "            if isinstance(generator, DiffusionModelUNet):\n",
    "                gen_images = generator(x=real_A, timesteps=torch.zeros(real_A.shape[0], device=device))\n",
    "            else:\n",
    "                gen_images = generator(real_A)  # [B, 1, H, W]\n",
    "\n",
    "            \n",
    "        for i in range(real_A.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": real_A[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": real_B[i].cpu(),\n",
    "                \"prediction\": gen_images[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "            \n",
    "        if just_one_batch:\n",
    "            break\n",
    "        wandb.log({\"prediction_progress\": idx})\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "# diffusion model\n",
    "@torch.no_grad()\n",
    "def diff_generate_and_save_predictions(model: nn.Module, inferer: DiffusionInferer, test_loader: DataLoader, device: str, output_dir: str = OUTPUT_DIR, just_one_batch: bool = False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        t1_cond = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2_target = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        noise = torch.randn_like(t2_target).to(device)          # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        gen_image = inferer.sample(input_noise=noise, diffusion_model=model,\n",
    "                                   scheduler=inferer.scheduler, mode='concat', conditioning=t1_cond)\n",
    "\n",
    "        for i in range(t1_cond.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1_cond[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2_target[i].cpu(),\n",
    "                \"prediction\": gen_image[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "        if just_one_batch:\n",
    "            break\n",
    "        wandb.log({\"prediction_progress\": idx})\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "# flow matching direct\n",
    "@torch.no_grad()\n",
    "def direct_generate_and_save_predictions(model, test_loader, device, output_dir=OUTPUT_DIR, just_one_batch=False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "        t1 = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2 = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        x_gen = direct_generate(model, x_T=t1, n_steps=300)\n",
    "        # print(t2.shape, x_gen.shape)\n",
    "\n",
    "        for i in range(t1.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2[i].cpu(),\n",
    "                \"prediction\": x_gen[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "        if just_one_batch:\n",
    "            break\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "#flow matching noise\n",
    "@torch.no_grad()\n",
    "def noise_generate_and_save_predictions(model, test_loader, device, output_dir=OUTPUT_DIR, just_one_batch=False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    all_outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Generating Predictions\")):\n",
    "        t1 = batch[\"t1\"].to(device)           # [B, 1, H, W]\n",
    "        t2 = batch[\"t2\"].to(device)           # [B, 1, H, W]\n",
    "        filenames = batch[\"filename\"]         # list of strings (length B)\n",
    "\n",
    "        x_gen = noise_generate(model, x_cond=t1, n_steps=300)\n",
    "        # print(t2.shape, x_gen.shape)\n",
    "\n",
    "        for i in range(t1.size(0)):\n",
    "            sample = {\n",
    "                \"filename\": filenames[i],\n",
    "                \"input\": t1[i].cpu(),         # torch.Tensor [1, H, W]\n",
    "                \"target\": t2[i].cpu(),\n",
    "                \"prediction\": x_gen[i].cpu()\n",
    "            }\n",
    "\n",
    "            torch.save(sample, os.path.join(output_dir, f\"{filenames[i]}.pt\"))\n",
    "            all_outputs.append(sample)\n",
    "        if just_one_batch:\n",
    "            break\n",
    "        wandb.log({\"prediction_progress\": idx})\n",
    "\n",
    "    return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e745bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(fm_direct_path, map_location='cpu')\n",
    "fm_direct.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint = torch.load(fm_noise_path, map_location='cpu')\n",
    "fm_noise.load_state_dict(checkpoint['model_state_dict'])\n",
    "# --------------------------------------\n",
    "num_train_timesteps = 1000\n",
    "checkpoint = torch.load(controldiff_path, map_location='cpu')\n",
    "control_diff.load_state_dict(checkpoint['model_state_dict'])\n",
    "control_scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps)\n",
    "control_diff_inferer = DiffusionInferer(control_scheduler)\n",
    "\n",
    "checkpoint = torch.load(controlnet_path, map_location='cpu')\n",
    "controlnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "controlnet_inferer = ControlNetDiffusionInferer(control_scheduler)\n",
    "# --------------------------------------\n",
    "\n",
    "checkpoint = torch.load(pixgen_path, map_location='cpu')\n",
    "pixgen.load_state_dict(checkpoint['generator_state_dict'])\n",
    "\n",
    "checkpoint = torch.load(diffusion_path, map_location='cpu')\n",
    "dm.load_state_dict(checkpoint['model_state_dict'])\n",
    "dm_scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps)\n",
    "dm_inferer = DiffusionInferer(dm_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04112ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=(5, 3, 5, 3), fill=0),\n",
    "    transforms.ToTensor(),  # Normalize to [0, 1]\n",
    "])\n",
    "test_dataset = UnifiedBrainDataset(root_dir=DATAPATH, transform=transform, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2071cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#direct\n",
    "direct_generate_and_save_predictions(\n",
    "    model=fm_direct, \n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
